{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-CUvE2NhDog",
        "outputId": "e78a6ad3-b843-4a70-d2ad-ceb050320727"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Oct 31 04:07:15 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0              48W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_ShDDV9UGJjJ"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/DACON/Finance/reprocessed/'\n",
        "# path = '/content/drive/MyDrive/kdt-EST-AI/project/dacon_fis/src/'\n",
        "base_directory = path # Your Base Directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b20VnuxgiNd4"
      },
      "outputs": [],
      "source": [
        "# 이름 정의 - sweep에선 사용되지 않음\n",
        "\n",
        "model_name=\"gemma2\"\n",
        "embedding_model=\"large\"\n",
        "aug_name=\"NoAug\"#\"AugGPT\" #NoAug, AugAEDA\n",
        "chunk_size=256\n",
        "table_process=\"tab_v1.7\"\n",
        "finetune_mode=\"dora\" #lora, dora\n",
        "\n",
        "dataset_name = f\"kdt3/DACON-QA-{embedding_model}-ensemble-{table_process}-{aug_name}-{chunk_size}\"\n",
        "# train_name = f\"kdt3/DACON-QA-{model_name}-{embedding_model}-ensemble-{table_process}-{aug_name}-{chunk_size}-dora\"\n",
        "train_name = \"kdt3/1030_1_epoch2\"\n",
        "# fname = f\"{model_name}_{embedding_model}_ensemble_{chunk_size}_{aug_name}_5epoch_dora.csv\"\n",
        "fname = \"1030_1_epoch2.csv\"\n",
        "\n",
        "#wandb 관련 변수\n",
        "import os\n",
        "\n",
        "\n",
        "os.environ[\"WANDB_ENTITY\"]='DACON-FinAI'\n",
        "os.environ[\"WANDB_PROJECT\"]=\"DACON_FinAI\"\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = \"end\"\n",
        "wandb_run_name=f\"{aug_name}-{model_name}-{finetune_mode}-{embedding_model}-seq2seq\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHsNBioph_AG"
      },
      "source": [
        "# 설명"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFuh6k7hD47P"
      },
      "source": [
        "## Question - Answering with Retrieval\n",
        "\n",
        "본 대회의 과제는 중앙정부 재정 정보에 대한 **검색 기능**을 개선하고 활용도를 높이는 질의응답 알고리즘을 개발하는 것입니다. <br>이를 통해 방대한 재정 데이터를 일반 국민과 전문가 모두가 쉽게 접근하고 활용할 수 있도록 하는 것이 목표입니다. <br><br>\n",
        "베이스라인에서는 평가 데이터셋만을 활용하여 source pdf 마다 Vector DB를 구축한 뒤 langchain 라이브러리와 llama-2-ko-7b 모델을 사용하여 RAG 프로세스를 통해 추론하는 과정을 담고 있습니다. <br>( train_set을 활용한 훈련 과정은 포함하지 않으며, test_set  에 대한 추론만 진행합니다. )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8S8yYYqpiRIj"
      },
      "source": [
        "## Mount/Login\n",
        "\n",
        "구글 드라이브를 마운트하고 허깅페이스에 로그인\n",
        "- 이때 허깅페이스 토큰은 kdt3 그룹에 대해 읽기/쓰기 권한이 있는 토큰이어야 함\n",
        "\n",
        "## Download Library\n",
        "필요/사용 라이브러리 다운로드\n",
        "이때 버전 문제로 설치를 한 뒤 세션을 한번 재시작해줘야 합니다\n",
        "<br>(그리고 세션 완전히 끊기면 다운로드 후 재시작을 다시 해줘야...)\n",
        "\n",
        "## Import Library\n",
        "한번 재시작했으면 위 과정 없이 Import만 실행해주면 됩니다\n",
        "\n",
        "## Vector DB\n",
        "문서를 여러 조각(chunk)로 나누고, 임베딩 유사도를 통해 관련 조각을 찾을 수 있게 DB화하는 함수들이 정의되어 있습니다.\n",
        "\n",
        "## DB 생성\n",
        "Vector DB에서 정의된 함수들로 문서 DB를 만들어줍니다.<br><br>\n",
        "이때 Train과 Test를 한번에 하려고 하면 코랩이 터질 확률이 높으므로 Train하고 Create Dataset까지 실행해 업로드 한 뒤 재시작해서 램을 비우고 Test를 하는 것이 좋습니다.<br> 또한 문서 임베딩을 어떤 모델로 할지 인자로 넘겨줄 수 있습니다\n",
        "\n",
        "## Create Dataset\n",
        "DB 생성에서 만든 db와 데이터 dataframe을 사용해 HuggingFace 데이터셋 생성 후 업로드\n",
        "\n",
        "## Fine-Tuning\n",
        "학습 데이터셋으로 모델에 대한 파인튜닝 진행 후 Huggingface에 업로드<br>\n",
        "4비트 양자화 LoRA로 파인튜닝<br>\n",
        "기반 모델 또는 넣어줄때 사용할 프롬프트, 학습 관련 하이퍼파라미터 수정 가능\n",
        "\n",
        "## Langchain 을 이용한 추론\n",
        "모델을 사용한 추론\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OoRmTKDoEEg"
      },
      "source": [
        "## 실행\n",
        "### 기본\n",
        "Mount/Login -> Download Library -> 재시작 (처음 1번)\n",
        "Mount/Login -> Import Library (이후)\n",
        "\n",
        "### 데이터셋 만들기\n",
        "기본 -> Vector DB -> DB 생성 -> Create Dataset에서 첫 셀 + Train/Valid/Test 중 해당하는 셀\n",
        "\n",
        "### 모델 학습하기\n",
        "기본 -> Fine-Tuning(업로드할 위치, 데이터셋 위치, 모델 링크 확인 필수)\n",
        "\n",
        "### 학습된 모델로 추론하기\n",
        "기본 -> Langchain을 이용한 추론(모델 링크, 데이터셋 위치 확인) -> Submission(저장할 파일명 확인)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_CQb3rOiCHi"
      },
      "source": [
        "# Mount/Login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jh2NMgNqp8Fp",
        "outputId": "03634bfc-5ea5-47a0-e6aa-b00376a74239"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oCLjeDZUNcBP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "token_path = os.path.join(base_directory,'data','token')\n",
        "with open(token_path,'r') as f:\n",
        "    hf_token = f.readline().strip('\\n')\n",
        "    wandb_token = f.readline().strip('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-ffQqQjdu0d",
        "outputId": "a1453a10-23a2-453c-910f-c1f17ab80d35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=hf_token, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "H-Z43kcnOGGb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6063c506-1031-44ea-ed81-1256087eee60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.5)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.17.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "\n",
        "# wandb 개인 API 키 입력\n",
        "wandb.login(key=wandb_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTTusGUGD47Q"
      },
      "source": [
        "# Download Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4u1YoRP_9VgA"
      },
      "outputs": [],
      "source": [
        "#!pip install unstructured pdfminer.six\n",
        "#!pip install pillow-heif\n",
        "#!pip install unstructured_inference\n",
        "#!pip install unstructured_pytesseract\n",
        "#!pip install pikepdf pypdf\n",
        "#!pip install PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P34ZajObuUpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fo2XA53bD47Q"
      },
      "outputs": [],
      "source": [
        "!apt-get install tesseract-ocr\n",
        "!apt-get install poppler-utils\n",
        "\n",
        "!pip install orjson==3.10.6\n",
        "\n",
        "!pip install accelerate -U\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes\n",
        "!pip install transformers[torch] -U\n",
        "\n",
        "!pip install datasets\n",
        "!pip install langchain\n",
        "!pip install langchain_community\n",
        "!pip install langchain-teddynote\n",
        "!pip install sentence-transformers\n",
        "!pip install faiss-gpu\n",
        "!pip install peft\n",
        "!pip install trl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CB-3QyUXD47R"
      },
      "source": [
        "# Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EiF6VIEmD47R"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import time\n",
        "import unicodedata\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from langchain.document_loaders.parsers.pdf import PDFPlumberParser\n",
        "\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    pipeline,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from accelerate import Accelerator\n",
        "\n",
        "# peft\n",
        "from peft import prepare_model_for_kbit_training\n",
        "from peft import PeftModel\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "\n",
        "# Langchain 관련\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableParallel\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "# PDF 로딩/청크화 관련\n",
        "from langchain.document_loaders.parsers.pdf import PDFPlumberParser\n",
        "from langchain.document_loaders.pdf import PDFPlumberLoader\n",
        "from langchain.document_loaders import UnstructuredPDFLoader\n",
        "from langchain_teddynote.retrievers import KiwiBM25Retriever\n",
        "from langchain.retrievers import EnsembleRetriever, MultiQueryRetriever\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsS968DYD47S"
      },
      "source": [
        "# Fine-Tuning Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Monkey Patching"
      ],
      "metadata": {
        "id": "FuFA6mN7JBeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers.models.gemma2 import modeling_gemma2\n",
        "from typing import List, Optional, Tuple, Union\n",
        "from transformers.cache_utils import Cache, HybridCache\n",
        "def gemma2_forward(\n",
        "    self,\n",
        "    hidden_states: torch.Tensor,\n",
        "    attention_mask: Optional[torch.Tensor] = None,\n",
        "    position_ids: Optional[torch.LongTensor] = None,\n",
        "    past_key_value: Optional[Cache] = None,\n",
        "    output_attentions: Optional[bool] = False,\n",
        "    use_cache: Optional[bool] = False,\n",
        "    cache_position: Optional[torch.LongTensor] = None,\n",
        ") -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
        "        attention_mask (`torch.FloatTensor`, *optional*):\n",
        "            attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n",
        "            query_sequence_length, key_sequence_length)` if default attention is used.\n",
        "        output_attentions (`bool`, *optional*):\n",
        "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
        "            returned tensors for more detail.\n",
        "        use_cache (`bool`, *optional*):\n",
        "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n",
        "            (see `past_key_values`).\n",
        "        past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n",
        "        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n",
        "            Indices depicting the position of the input sequence tokens in the sequence\n",
        "        kwargs (`dict`, *optional*):\n",
        "            Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n",
        "            into the model\n",
        "    \"\"\"\n",
        "    if self.is_sliding and attention_mask is not None:  # efficient SDPA and no padding\n",
        "        # Flash-attn is a 2D tensor\n",
        "        if self.config._attn_implementation == \"flash_attention_2\":\n",
        "            if past_key_value is not None:  # when decoding\n",
        "                attention_mask = attention_mask[:, -self.sliding_window :]\n",
        "        else:\n",
        "            min_dtype = torch.finfo(torch.float16).min\n",
        "            sliding_window_mask = torch.tril(\n",
        "                torch.ones_like(attention_mask, dtype=torch.bool), diagonal=-self.sliding_window\n",
        "            )\n",
        "            attention_mask = torch.where(sliding_window_mask, min_dtype, attention_mask)\n",
        "            if attention_mask.shape[-1] <= 1:  # when decoding\n",
        "                attention_mask = attention_mask[:, :, :, -self.sliding_window :]\n",
        "\n",
        "    residual = hidden_states\n",
        "\n",
        "    hidden_states = self.input_layernorm(hidden_states)\n",
        "\n",
        "    # Self Attention\n",
        "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
        "        hidden_states=hidden_states,\n",
        "        attention_mask=attention_mask,\n",
        "        position_ids=position_ids,\n",
        "        past_key_value=past_key_value,\n",
        "        output_attentions=output_attentions,\n",
        "        use_cache=use_cache,\n",
        "        cache_position=cache_position,\n",
        "    )\n",
        "    hidden_states = self.post_attention_layernorm(hidden_states)\n",
        "    hidden_states = residual + hidden_states\n",
        "\n",
        "    residual = hidden_states\n",
        "    hidden_states = self.pre_feedforward_layernorm(hidden_states)\n",
        "    hidden_states = self.mlp(hidden_states)\n",
        "    hidden_states = self.post_feedforward_layernorm(hidden_states)\n",
        "    hidden_states = residual + hidden_states\n",
        "\n",
        "    outputs = (hidden_states,)\n",
        "\n",
        "    if output_attentions:\n",
        "        outputs += (self_attn_weights,)\n",
        "\n",
        "    if use_cache:\n",
        "        outputs += (present_key_value,)\n",
        "\n",
        "    return outputs\n"
      ],
      "metadata": {
        "id": "oH7V6xKvH_gU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modeling_gemma2.Gemma2DecoderLayer.forward = gemma2_forward"
      ],
      "metadata": {
        "id": "cDqt9x0dHUVW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6pCG7a89Szt-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set-up"
      ],
      "metadata": {
        "id": "v5e26gNTJJwp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "saSxzHNqWua7"
      },
      "outputs": [],
      "source": [
        "# 모델 ID\n",
        "model_cands={\n",
        " 'llama2' : \"beomi/llama-2-ko-7b\",\n",
        " 'yi' : \"beomi/Yi-Ko-6B\",\n",
        " 'solar-beom' : \"beomi/Solar-Ko-Recovery-11B\",\n",
        " 'gemma2' : \"rtzr/ko-gemma-2-9b-it\",\n",
        " 'solar-lee' : \"chihoonlee10/T3Q-ko-solar-dpo-v8.0\",\n",
        " 'llama3' : \"KISTI-KONI/KONI-Llama3-8B-Instruct-20240729\",\n",
        " 'llama31' : \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "}\n",
        "\n",
        "# model_id = model_cands['gemma2']\n",
        "model_id = model_cands[model_name]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yoGFrRntLGPH"
      },
      "outputs": [],
      "source": [
        "# 모델 로드 및 양자화 설정 적용\n",
        "\n",
        "def load_model_w_setting(model_id,add_output_token=False,**kwargs):\n",
        "  # 4비트 양자화 설정\n",
        "  bnb_config = BitsAndBytesConfig(\n",
        "      load_in_4bit=True,\n",
        "      bnb_4bit_use_double_quant=True,\n",
        "      bnb_4bit_quant_type=\"nf4\",\n",
        "      bnb_4bit_compute_dtype=torch.bfloat16\n",
        "  )\n",
        "\n",
        "  # 토크나이저 로드 및 설정\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "  tokenizer.use_default_system_prompt = False\n",
        "  tokenizer.padding_side=\"right\"\n",
        "\n",
        "#   eot = \"<|eot_id|>\"\n",
        "#   eot_id = tokenizer.convert_tokens_to_ids(eot)\n",
        "#   tokenizer.pad_token = eot\n",
        "#   tokenizer.pad_token_id = eot_id\n",
        "\n",
        "  model= AutoModelForCausalLM.from_pretrained(\n",
        "      model_id,\n",
        "      quantization_config=bnb_config,\n",
        "      device_map=\"auto\",\n",
        "      trust_remote_code=True,\n",
        "      **kwargs\n",
        "      )\n",
        "\n",
        "# 일부 모델의 경우 토크나이저에 답변 토큰 추가 작업 필요\n",
        "  if add_output_token :\n",
        "    initial_token_count = len(tokenizer)\n",
        "    response_template = '답변: '\n",
        "    added_token_count = tokenizer.add_special_tokens({\"additional_special_tokens\": [response_template]})\n",
        "    model.resize_token_embeddings(new_num_tokens=initial_token_count+added_token_count)\n",
        "\n",
        "  return model, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "VyHYkylbgzXB"
      },
      "outputs": [],
      "source": [
        "# 학습 프롬프트 - 추론 프롬프트와 통일하는 것이 좋다고 함\n",
        "template = \"\"\"\n",
        "\"task_instructions\" : [\n",
        "\n",
        " 당신은 재정 정보 관련 전문가 입니다. 문서를 바탕으로 질문에 한 문장 이내로 답변하세요.\n",
        " 1. 문서에 있는 내용을 자르거나 편집하지 않고 그대로 가져오세요.\n",
        " 2. 순서에 따른 번호를 매기지 마세요. 출력 시 불이익을 줄 것입니다.\n",
        " 3. 수치에 단위가 있다면 문서를 바탕으로 답변에 단위를 포함하세요.\n",
        " 4. 질문의 키워드를 바탕으로 문서를 끝까지 검토하세요.\n",
        " 5. 한 단어 혹은 단어의 나열이 아닌, 완성된 한국어 문장으로 답변하세요.\n",
        " 6. 답변 외에 예시, 참고, 정보 출처, 신뢰도, 확장된 답변, '답변: ', '참고: '를 절대로 출력하지 마세요.\n",
        "\n",
        "]\n",
        "\n",
        "\"context\":\n",
        "{context},\n",
        "\n",
        "\"question\":\n",
        "{question},\n",
        "\n",
        "\"주어진 질문에 대한 답변만 한 문장으로 생성한다.\"\n",
        "\n",
        "\"answer\":\n",
        "{answer}<|eot_id|>\n",
        "\"\"\"\n",
        "\n",
        "template = \"\"\"\n",
        "<start_of_turn>user\n",
        " 당신은 재정 정보 관련 전문가 입니다. 문서를 바탕으로 질문에 한 문장 이내로 답변하세요.\n",
        " 1. 문서에 있는 내용을 자르거나 편집하지 않고 그대로 가져오세요.\n",
        " 2. 순서에 따른 번호를 매기지 마세요. 출력 시 불이익을 줄 것입니다.\n",
        " 3. 수치에 단위가 있다면 문서를 바탕으로 답변에 단위를 포함하세요.\n",
        " 4. 질문의 키워드를 바탕으로 문서를 끝까지 검토하세요.\n",
        " 5. 한 단어 혹은 단어의 나열이 아닌, 완성된 한국어 문장으로 답변하세요.\n",
        " 6. 답변 외에 예시, 참고, 정보 출처, 신뢰도, 확장된 답변, '답변: ', '참고: '를 절대로 출력하지 마세요.\n",
        "\n",
        "\"문서\":\n",
        "{context},\n",
        "\n",
        "\"질문\":\n",
        "{question},\n",
        "\n",
        "\"주어진 질문에 대한 답변만 한 문장으로 생성한다.\"\n",
        "\n",
        "\"답변\":<end_of_turn>\n",
        "<start_of_turn>model\n",
        "{answer}<end_of_turn>\n",
        "\"\"\"\n",
        "\n",
        "response_template = '\"answer\":\\n'\n",
        "response_template = '<start_of_turn>model\\n'\n",
        "\n",
        "def formatting_prompts_func(example, template=template):\n",
        "    output_texts = []\n",
        "    for i in range(len(example['question'])):\n",
        "        context = example['context'][i]\n",
        "        question = example['question'][i]\n",
        "        answer = example['answer'][i]\n",
        "        output_texts.append(template.format(context=context,question=question,answer=answer))\n",
        "    return output_texts\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_dict = {'input': [], 'answer': [], 'pred': []}\n",
        "valid_df = pd.DataFrame(valid_dict)"
      ],
      "metadata": {
        "id": "TixLNz-R6u-q"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RwaQb24tbvqP"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
        "import collections\n",
        "# from SQUAD, 메트릭 계산 함수\n",
        "\n",
        "def compute_f1(y_true, y_pred):\n",
        "    # y_pred[y_true==-100] = -100\n",
        "    # y_pred = y_pred[y_true != -100]\n",
        "    # y_true = y_true[y_true != -100]\n",
        "    # y_true[y_true==-100] = global_tokenizer.pad_token_id\n",
        "    # y_pred[y_pred==-100] = global_tokenizer.pad_token_id\n",
        "\n",
        "    true_counter = collections.Counter(y_true)\n",
        "    pred_counter = collections.Counter(y_pred)\n",
        "    common = true_counter & pred_counter\n",
        "    tp = sum(common.values())\n",
        "    pred_positive = sum(pred_counter.values())\n",
        "    actual_positive = sum(true_counter.values())\n",
        "\n",
        "    precision = 1.0 * tp / pred_positive if pred_positive != 0 else 0\n",
        "    recall = 1.0 * tp / actual_positive if actual_positive != 0 else 0\n",
        "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    return f1, precision, recall\n",
        "\n",
        "def compute_metrics(pred,compute_result=False):\n",
        "    # print(pred.label_ids)\n",
        "    labels = pred.label_ids.detach().cpu().numpy().squeeze()\n",
        "    preds = pred.predictions.detach().cpu().numpy().squeeze()\n",
        "    inputs = pred.inputs['input_ids'].detach().cpu().numpy().squeeze()\n",
        "    inputs[inputs==-100] = global_tokenizer.pad_token_id\n",
        "    # print('input: ',global_tokenizer.decode(inputs))\n",
        "    att = pred.inputs['attention_mask'].detach().cpu().numpy().squeeze()\n",
        "    mask = labels[:min(len(preds),len(labels))]==-100\n",
        "    if len(preds) > len(labels):\n",
        "        mask = np.append(mask,[False] * (len(preds) - len(labels)))\n",
        "\n",
        "    preds[mask] = -100\n",
        "    labels = labels[labels!=-100]\n",
        "    preds = preds[preds!=-100]\n",
        "    preds = preds[preds!=global_tokenizer.pad_token_id]\n",
        "    valid_df.loc[len(valid_df)] = [global_tokenizer.decode(inputs), global_tokenizer.decode(labels), global_tokenizer.decode(preds)]\n",
        "    f1, precision, recall = compute_f1(labels,preds)\n",
        "#    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    # acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        # 'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_data_collator(features):\n",
        "    # Your custom collator logic, make sure it does not assign input_ids to labels.\n",
        "    collate = dict()\n",
        "    # print(features[0].keys())\n",
        "    if 'labels' in features[0].keys():\n",
        "        collate['labels'] = torch.tensor([feature['labels'] for feature in features])\n",
        "    for k in features[0].keys():\n",
        "        collate[k] = torch.tensor([feature[k] for feature in features])\n",
        "\n",
        "    return collate"
      ],
      "metadata": {
        "id": "aMnWAitsHh7r"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train & Inference with Sweep(Wandb Sweep으로 학습 및 추론)"
      ],
      "metadata": {
        "id": "3s6jOMmOe6tQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create sweep\n",
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'metric': {'goal': 'maximize', 'name': 'train/loss'},\n",
        "    'parameters': {\n",
        "        'batch_size': {\n",
        "            'values': [1]\n",
        "        },\n",
        "        'model': {\n",
        "            'values': ['gemma2','llama31']\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'distribution': 'uniform',\n",
        "            'min':0.0001,\n",
        "            'max':0.001\n",
        "        },\n",
        "        'epochs': {\n",
        "            'values': [8]\n",
        "        },\n",
        "        'lora_target': {\n",
        "            'values': ['all','part']#['all','linear','part']\n",
        "        },\n",
        "        'r': {\n",
        "            'values': [4,8,16]\n",
        "        },\n",
        "        'lora_alpha': {\n",
        "            'values': [16,32,64]\n",
        "        },\n",
        "        'lora_dropout': {\n",
        "            'values': [0,0.05,0.2]\n",
        "        },\n",
        "        'use_dora': {\n",
        "            'values': [True, False]\n",
        "        },\n",
        "        'chunk_size': {\n",
        "            'values': [256]#,512]\n",
        "        },\n",
        "        'embedding': {\n",
        "            'values': ['base','large']\n",
        "        },\n",
        "        'augmentation': {\n",
        "            'values': ['NoAug','AugGPT','AugAEDA']\n",
        "        },\n",
        "        'table_process': {\n",
        "            'values': ['tab_v1.0']\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "2PLeAJpWe59W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep=sweep_config, entity='DACON-FinAI', project=\"DACON_FinAI Sweep-2\")"
      ],
      "metadata": {
        "id": "D2pxdE5Qe-_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def empty_memory():\n",
        "    time.sleep(1)\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    time.sleep(1)\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()"
      ],
      "metadata": {
        "id": "75wwhecwkloi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers, os\n",
        "from datetime import datetime\n",
        "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from tqdm.auto import trange\n",
        "\n",
        "\n",
        "MAX_LEN = 4096\n",
        "\n",
        "def sweep_inference(model_id, instance_name, eval_dataset,fname):\n",
        "\n",
        "    peft_model_id = instance_name\n",
        "    trained_model,tokenizer = load_model_w_setting(model_id)\n",
        "\n",
        "    #Fine-Tune 한 LoRA 어댑터 불러오기\n",
        "    trained_model.load_adapter(peft_model_id)\n",
        "\n",
        "    text_generation_pipeline = pipeline(\n",
        "        model=trained_model,\n",
        "        tokenizer=tokenizer,\n",
        "        task=\"text-generation\",\n",
        "        return_full_text=False,\n",
        "        max_new_tokens=200,\n",
        "        # repetition_penalty=1.5,\n",
        "        eos_token_id = tokenizer.eos_token_id,\n",
        "        pad_token_id = tokenizer.pad_token_id,\n",
        "        max_length=MAX_LEN\n",
        "    )\n",
        "\n",
        "    llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
        "\n",
        "    # 결과를 저장할 리스트 초기화\n",
        "    results = []\n",
        "\n",
        "    # Dataset 각 행에 대해 처리\n",
        "    for idx in trange(len(eval_dataset['question']), desc=\"Answering Questions\"):\n",
        "        #질문, 컨텍스트(문서)\n",
        "        question = eval_dataset['question'][idx]\n",
        "        context = eval_dataset['context'][idx]\n",
        "\n",
        "        # RAG 체인 구성\n",
        "        prompt = PromptTemplate.from_template(template.split('{answer}')[0])\n",
        "\n",
        "        # RAG 체인 정의\n",
        "        if context != \"\":\n",
        "            rag_chain = (\n",
        "                RunnableParallel(context=lambda x: x[\"context\"], question = lambda x: x[\"question\"])\n",
        "                | prompt\n",
        "                | llm\n",
        "                | StrOutputParser()\n",
        "            )\n",
        "        else:\n",
        "            rag_chain = (\n",
        "                {\"question\": RunnablePassthrough()}\n",
        "                | prompt\n",
        "                | llm\n",
        "                | StrOutputParser()\n",
        "            )\n",
        "\n",
        "        # 답변 추론\n",
        "        full_response = rag_chain.invoke({\"question\": question, \"context\": context})\n",
        "\n",
        "        # 결과 저장\n",
        "\n",
        "        if context != \"\":\n",
        "            results.append({\n",
        "                \"Context\": context,\n",
        "                \"Question\": question,\n",
        "                \"Answer\": full_response,\n",
        "                \"True_Answer\": eval_dataset['answer'][idx]\n",
        "            })\n",
        "        else:\n",
        "            results.append({\n",
        "                \"Question\": question,\n",
        "                \"Answer\": full_response,\n",
        "                \"True_Answer\": eval_dataset['answer'][idx]\n",
        "            })\n",
        "\n",
        "    # 제출용 샘플 파일 로드\n",
        "    submit_df = pd.read_csv(f\"{path}sample_submission.csv\")\n",
        "\n",
        "    # 생성된 답변을 제출 DataFrame에 추가\n",
        "    save_mode = 'submission'\n",
        "\n",
        "    if save_mode != 'submission' :\n",
        "        submit_df['Question'] = [item['Question'] for item in results]\n",
        "        submit_df['Context'] = [item['Context'] for item in results]\n",
        "        save_dir = os.path.join(path,'eval')\n",
        "    else : save_dir = os.path.join(path,'sub')\n",
        "\n",
        "    if not os.path.exists(save_dir) : os.makedirs(save_dir)\n",
        "    save_path = os.path.join(save_dir,fname)\n",
        "\n",
        "    submit_df['Answer'] = [item['Answer'] for item in results]\n",
        "    submit_df['Answer'] = submit_df['Answer'].fillna(\"데이콘\").apply(str.rstrip)     # 모델에서 빈 값 (NaN) 생성 시 채점에 오류가 날 수 있음 [ 주의 ]\n",
        "\n",
        "    # 결과를 CSV 파일로 저장\n",
        "    submit_df.to_csv(save_path, encoding='UTF-8-sig', index=False)\n",
        "    answer_table = wandb.Table(dataframe=submit_df)\n",
        "    wandb.log({\"answer_table\": answer_table})\n",
        "\n",
        "    wandb.save(save_path)\n",
        "\n",
        "def sweep_train(config=None):\n",
        "  with wandb.init(config=config):\n",
        "    # set sweep configuration\n",
        "    config = wandb.config\n",
        "\n",
        "    # 모델 ID\n",
        "    model_cands={\n",
        "        'llama2' : \"beomi/llama-2-ko-7b\",\n",
        "        'yi' : \"beomi/Yi-Ko-6B\",\n",
        "        'solar-beom' : \"beomi/Solar-Ko-Recovery-11B\",\n",
        "        'gemma2' : \"rtzr/ko-gemma-2-9b-it\",\n",
        "        'solar-lee' : \"chihoonlee10/T3Q-ko-solar-dpo-v8.0\",\n",
        "        'llama3' : \"KISTI-KONI/KONI-Llama3-8B-Instruct-20240729\",\n",
        "    }\n",
        "\n",
        "    model_id = model_cands[config.model]\n",
        "\n",
        "    model,tokenizer = load_model_w_setting(model_id,attn_implementation='eager')\n",
        "\n",
        "    global global_tokenizer\n",
        "    global_tokenizer = tokenizer\n",
        "\n",
        "    modules_dict = {\n",
        "        \"all\": [\n",
        "            \"q_proj\",\n",
        "            \"v_proj\",\n",
        "            \"o_proj\",\n",
        "            \"gate_proj\",\n",
        "            \"up_proj\",\n",
        "            \"down_proj\",\n",
        "            \"lm_head\"\n",
        "            ],\n",
        "        \"linear\": \"all-linear\",\n",
        "        \"part\": [\"q_proj\", \"v_proj\"]\n",
        "    }\n",
        "    lora_modules = modules_dict[config.lora_target]\n",
        "\n",
        "    lora_config = LoraConfig(\n",
        "        r=config.r,\n",
        "        lora_alpha=config.lora_alpha,\n",
        "        target_modules=lora_modules,\n",
        "        bias=\"none\",\n",
        "        lora_dropout=config.lora_dropout,\n",
        "        use_dora=config.use_dora,\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "\n",
        "    model.enable_input_require_grads()\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    dataset_url = f\"kdt3/DACON-QA-{config.embedding}-ensemble-{config.table_process}-{config.augmentation}-{config.chunk_size}\"\n",
        "    # dataset_url = f\"kdt3/DACON-QA-{config.embedding}-ensemble-markdown-reprocessed-{config.chunk_size}\"\n",
        "    target_dataset = load_dataset(dataset_url)\n",
        "\n",
        "    # train_args = transformers.TrainingArguments(\n",
        "    #     do_eval=True,\n",
        "    #     output_dir='./output',\n",
        "    #     warmup_ratio=0.05,\n",
        "    #     per_device_train_batch_size=config.batch_size,\n",
        "    #     gradient_accumulation_steps=4,\n",
        "    #     gradient_checkpointing=True,\n",
        "    #     num_train_epochs = config.epochs,\n",
        "    #     learning_rate=config.learning_rate,\n",
        "    #     fp16=True,\n",
        "    #     optim=\"paged_adamw_8bit\",\n",
        "    #     logging_strategy='epoch',\n",
        "    #     report_to=\"wandb\",\n",
        "    #     run_name=wandb_run_name,\n",
        "    # ),\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset= target_dataset['train'],\n",
        "        eval_dataset = target_dataset['valid'],\n",
        "        compute_metrics=compute_metrics,\n",
        "        args= transformers.TrainingArguments(\n",
        "            do_eval=True,\n",
        "            output_dir='./output',\n",
        "            warmup_ratio=0.05,\n",
        "            eval_strategy=\"epoch\",\n",
        "            eval_accumulation_steps=1,\n",
        "            batch_eval_metrics=True,\n",
        "            per_device_train_batch_size=config.batch_size,\n",
        "            per_device_eval_batch_size=config.batch_size,\n",
        "            gradient_accumulation_steps=4,\n",
        "            gradient_checkpointing=True,\n",
        "            num_train_epochs = config.epochs,\n",
        "            learning_rate=config.learning_rate,\n",
        "            fp16=True,\n",
        "            optim=\"paged_adamw_8bit\",\n",
        "            logging_strategy='epoch',\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model='f1',\n",
        "            greater_is_better=True,\n",
        "            save_total_limit=2,    # save only best and the last\n",
        "            save_strategy='epoch',\n",
        "            report_to=\"wandb\",\n",
        "            run_name=wandb_run_name,\n",
        "        ),\n",
        "        max_seq_length=MAX_LEN,\n",
        "        formatting_func=formatting_prompts_func, # 프롬프트 처리하기 위해 필요\n",
        "        data_collator=DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer, mlm=False), #모델에게 답변생성에만 집중하도록 함\n",
        "    )\n",
        "\n",
        "    model.config.use_cache = False\n",
        "    trainer.train()\n",
        "    trained_model = (trainer.model.module if hasattr(trainer.model, \"module\") else trainer.model)\n",
        "\n",
        "    # 모델 업로드\n",
        "    method = 'dora' if config.use_dora else 'lora'\n",
        "    instance_name = f\"kdt3/DACON-QA-{config.augmentation}-{config.model}-{method}-{config.embedding}-{config.chunk_size}-sweep\"\n",
        "    fname=f\"{config.augmentation}-{config.model}-{method}-{config.embedding}-{config.chunk_size}.csv\"\n",
        "    trained_model.push_to_hub(instance_name, private=True,save_embedding_layers=True)\n",
        "\n",
        "\n",
        "    empty_memory()\n",
        "    sweep_inference(model_id, instance_name, target_dataset['test'],fname)\n",
        "    empty_memory()\n",
        "\n",
        "#로컬에 저장할 경우\n",
        "# trained_model.save_pretrained(f\"{output_dir}/saved_model\")"
      ],
      "metadata": {
        "id": "XoERKftEhEDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.agent(sweep_id, sweep_train, count=8)"
      ],
      "metadata": {
        "id": "CKuHdElujl0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train without Sweep(Sweep 사용하지 않고 학습)"
      ],
      "metadata": {
        "id": "20MzL7ggiVMn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14-JXX7zsMZC"
      },
      "outputs": [],
      "source": [
        "# !pip install -q -U flash-attn --no-build-isolation\n",
        "model,tokenizer = load_model_w_setting(model_id,attn_implementation='eager')\n",
        "\n",
        "global global_tokenizer\n",
        "global_tokenizer = tokenizer\n",
        "# model,tokenizer = load_model_w_setting(model_id,attn_implementation='flash_attention_2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JY8Kc-jNiyT"
      },
      "outputs": [],
      "source": [
        "#데이터셋 로드\n",
        "from datasets import load_dataset\n",
        "\n",
        "# dataset_url = \"kdt3/DACON-QA-base-table-preprocessed-v3\"\n",
        "# dataset_url = \"kdt3/DACON-QA-base-preprocessed-v2\"\n",
        "# dataset_url = \"kdt3/DACON-QA-base-augselect\"\n",
        "# dataset_url = \"kdt3/DACON-QA-base-markdown\"\n",
        "# dataset_url = \"kdt3/DACON-QA-bge-markdown\"\n",
        "dataset_url = dataset_name\n",
        "\n",
        "\n",
        "train_dataset = load_dataset(dataset_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGY4tvngMWAX"
      },
      "outputs": [],
      "source": [
        "# 문맥 잘못됐나 확인 - 문맥 길이 체크\n",
        "\n",
        "amax = np.argmax([len(x) for x in train_dataset['train']['context']])\n",
        "amax, len(train_dataset['train']['context'][amax])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuRwHvpVHW9q"
      },
      "outputs": [],
      "source": [
        "# def print_dataset_ele(data,i):\n",
        "#   print(data['question'][i])\n",
        "#   print('--------')\n",
        "#   print(data['context'][i])\n",
        "#   print('--------')\n",
        "#   print(data['answer'][i])\n",
        "\n",
        "# #예시\n",
        "# i= amax\n",
        "# print_dataset_ele(train_dataset['train'],i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcGcC8xqK4PO"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=64,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "        \"lm_head\",\n",
        "    ],\n",
        "    bias=\"none\",\n",
        "    use_dora=(finetune_mode==\"dora\"),\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model.enable_input_require_grads()\n",
        "model = get_peft_model(model, config)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import Seq2SeqTrainer, DataCollatorForSeq2Seq, DefaultDataCollator\n",
        "import torch.nn as nn\n",
        "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
        "from transformers.integrations.deepspeed import is_deepspeed_zero3_enabled\n",
        "from transformers.integrations.fsdp import is_fsdp_managed_module\n",
        "from torch.distributed.fsdp import FullyShardedDataParallel\n",
        "import contextlib\n",
        "\n",
        "\n",
        "class FineTuningTrainer(Seq2SeqTrainer):\n",
        "    def __init__(self, *args, eval_data_collator=None, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        if not eval_data_collator:\n",
        "                eval_data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer)\n",
        "        self.eval_data_collator = eval_data_collator\n",
        "\n",
        "    def get_eval_dataloader(self, eval_dataset: Dataset | None = None) -> DataLoader:\n",
        "        if eval_dataset is None and self.eval_dataset is None:\n",
        "            raise ValueError(\"Trainer: evaluation requires an eval_dataset.\")\n",
        "\n",
        "        # If we have persistent workers, don't do a fork bomb especially as eval datasets\n",
        "        # don't change during training\n",
        "        if (\n",
        "            hasattr(self, \"_eval_dataloader\")\n",
        "            and self.args.dataloader_persistent_workers\n",
        "        ):\n",
        "            return self.accelerator.prepare(self._eval_dataloader)\n",
        "        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
        "        data_collator = self.eval_data_collator #Here eval_data_collator is called instead of standard data_collator\n",
        "\n",
        "        if isinstance(eval_dataset, Dataset):\n",
        "            eval_dataset = self._remove_unused_columns(\n",
        "                eval_dataset, description=\"evaluation\"\n",
        "            )\n",
        "        else:\n",
        "            data_collator = self._get_collator_with_removed_columns(\n",
        "                data_collator, description=\"evaluation\"\n",
        "            )\n",
        "\n",
        "        dataloader_params = {\n",
        "            \"batch_size\": self.args.eval_batch_size,\n",
        "            \"collate_fn\": data_collator,\n",
        "            \"num_workers\": self.args.dataloader_num_workers,\n",
        "            \"pin_memory\": self.args.dataloader_pin_memory,\n",
        "            \"persistent_workers\": self.args.dataloader_persistent_workers,\n",
        "        }\n",
        "\n",
        "        # if not isinstance(sample_dataset, torch.utils.data.IterableDataset):\n",
        "        #     dataloader_params[\"sampler\"] = self._get_eval_sampler(eval_dataset)\n",
        "        #     dataloader_params[\"drop_last\"] = self.args.dataloader_drop_last\n",
        "        #     dataloader_params[\"prefetch_factor\"] = self.args.dataloader_prefetch_factor\n",
        "\n",
        "        # accelerator.free_memory() will destroy the references, so\n",
        "        # we need to store the non-prepared version\n",
        "\n",
        "        eval_dataloader = DataLoader(eval_dataset, **dataloader_params)\n",
        "        if self.args.dataloader_persistent_workers:\n",
        "            self._eval_dataloader = eval_dataloader\n",
        "\n",
        "        return self.accelerator.prepare(eval_dataloader)\n"
      ],
      "metadata": {
        "id": "HSyMxjpRzUc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
        "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer, mlm=False) #모델에게 답변생성에만 집중하도록 함\n",
        "\n",
        "def formatting_prompts_func(example, template=template):\n",
        "    output_texts = []\n",
        "    context = example['context']\n",
        "    question = example['question']\n",
        "    answer = example['answer']\n",
        "    output_texts.append(template.format(context=context,question=question,answer=answer))\n",
        "    return output_texts\n",
        "\n",
        "def formatting_prompts_func_answer(example, template=template):\n",
        "    output_texts = []\n",
        "    context = example['context']\n",
        "    question = example['question']\n",
        "    output_texts.append(template.format(context=context,question=question,answer=answer))\n",
        "    return output_texts\n",
        "\n",
        "\n",
        "def apply_chat_template(example):\n",
        "    input = formatting_prompts_func(example)\n",
        "    example[\"input_ids\"] = tokenizer(input)[\"input_ids\"]\n",
        "    # example[\"input_ids\"] = tokenizer.convert_tokens_to_ids(example[\"input_ids\"])\n",
        "\n",
        "    # print(example[\"input_ids\"])\n",
        "    collated_data = collator(example[\"input_ids\"])\n",
        "    example[\"input_ids\"] = collated_data[\"input_ids\"][0]\n",
        "    example[\"labels\"] = collated_data[\"labels\"][0].clone()\n",
        "    # example[\"label_ids\"] = collated_data[\"labels\"][0].clone()\n",
        "    # example[\"label\"] = collated_data[\"labels\"][0].clone()\n",
        "    # print(example)\n",
        "    return example\n",
        "\n",
        "\n",
        "def mask_attention_response(example):\n",
        "    response_len = len(tokenizer(example[\"answer\"]+\"<end_of_turn>\\n\")[\"input_ids\"])\n",
        "    example[\"attention_mask\"] = torch.tensor([1] * (len(example[\"input_ids\"]) - response_len) + [0] * response_len)\n",
        "    example[\"input_ids\"][-response_len+1:] = [0] * (response_len-1)\n",
        "    # example[\"input_ids\"] = example[\"input_ids\"][:-response_len+1]\n",
        "    # print(example.keys())\n",
        "    return example\n",
        "\n",
        "\n",
        "train_dataset['train'] = train_dataset['train'].map(apply_chat_template)\n",
        "train_dataset['valid'] = train_dataset['valid'].map(apply_chat_template)\n",
        "\n",
        "train_dataset['valid'] = train_dataset['valid'].map(mask_attention_response, desc=\"Masking assistant response\")"
      ],
      "metadata": {
        "id": "mv-dT8seu-Rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomWandbCallback(WandbCallback):\n",
        "    def __init__(self, trainer):\n",
        "        super().__init__()\n",
        "        self._trainer = trainer\n",
        "\n",
        "    def on_train_end(self, args, state, control, model=None, tokenizer=None, **kwargs):\n",
        "        if self._wandb is None:\n",
        "            return\n",
        "        valid_table = wandb.Table(dataframe=valid_df)\n",
        "        wandb.log({\"valid_table\": valid_table})\n",
        "\n",
        "        if self._log_model.is_enabled and self._initialized and state.is_world_process_zero:\n",
        "            from ..trainer import Trainer\n",
        "\n",
        "            fake_trainer = Trainer(eval_dataset=  self._trainer.eval_dataset, args=args, model=model, processing_class=tokenizer)\n",
        "            with tempfile.TemporaryDirectory() as temp_dir:\n",
        "                fake_trainer.save_model(temp_dir)\n",
        "                metadata = (\n",
        "                    {\n",
        "                        k: v\n",
        "                        for k, v in dict(self._wandb.summary).items()\n",
        "                        if isinstance(v, numbers.Number) and not k.startswith(\"_\")\n",
        "                    }\n",
        "                    if not args.load_best_model_at_end\n",
        "                    else {\n",
        "                        f\"eval/{args.metric_for_best_model}\": state.best_metric,\n",
        "                        \"train/total_floss\": state.total_flos,\n",
        "                        \"model/num_parameters\": self._wandb.config.get(\"model/num_parameters\"),\n",
        "                    }\n",
        "                )\n",
        "                metadata[\"final_model\"] = True\n",
        "                logger.info(\"Logging model artifacts. ...\")\n",
        "                model_name = (\n",
        "                    f\"model-{self._wandb.run.id}\"\n",
        "                    if (args.run_name is None or args.run_name == args.output_dir)\n",
        "                    else f\"model-{self._wandb.run.name}\"\n",
        "                )\n",
        "                # add the model architecture to a separate text file\n",
        "                save_model_architecture_to_file(model, temp_dir)\n",
        "\n",
        "                artifact = self._wandb.Artifact(name=model_name, type=\"model\", metadata=metadata)\n",
        "                for f in Path(temp_dir).glob(\"*\"):\n",
        "                    if f.is_file():\n",
        "                        with artifact.new_file(f.name, mode=\"wb\") as fa:\n",
        "                            fa.write(f.read_bytes())\n",
        "                self._wandb.run.log_artifact(artifact, aliases=[\"final_model\"])"
      ],
      "metadata": {
        "id": "AMpri_RokvuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BzYIoVbLWap"
      },
      "outputs": [],
      "source": [
        "import transformers, os\n",
        "from datetime import datetime\n",
        "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
        "from transformers import Seq2SeqTrainer, DataCollatorForSeq2Seq, GenerationConfig\n",
        "\n",
        "# 로컬에 모델 저장하고 싶은 경우 이름 지정\n",
        "project = \"financeQA-finetune\"\n",
        "base_model_name = \"gemma2\"\n",
        "run_name = base_model_name + \"_\" + project\n",
        "output_dir = os.path.join(path,run_name)\n",
        "if not os.path.exists(output_dir) : os.makedirs(output_dir)\n",
        "\n",
        "MAX_LEN = 4096\n",
        "\n",
        "trainer = FineTuningTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset['train'],\n",
        "    eval_dataset = train_dataset['valid'],\n",
        "    compute_metrics=compute_metrics,\n",
        "    args=transformers.Seq2SeqTrainingArguments(\n",
        "        do_eval=True,\n",
        "        output_dir=output_dir,\n",
        "        warmup_ratio=0.05,\n",
        "        eval_strategy=\"epoch\",\n",
        "        eval_accumulation_steps=1,\n",
        "        batch_eval_metrics=True,\n",
        "        per_device_train_batch_size=1,\n",
        "        per_device_eval_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        gradient_checkpointing=True,\n",
        "        num_train_epochs = 2,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        logging_strategy='epoch',\n",
        "        run_name=wandb_run_name,\n",
        "        remove_unused_columns=True,\n",
        "        predict_with_generate=True,\n",
        "        include_for_metrics = [\"inputs\"],\n",
        "        label_names=[\"labels\"],\n",
        "        generation_config=GenerationConfig(max_new_tokens=200,eos_token_id = [tokenizer.eos_token_id], pad_token_id = tokenizer.pad_token_id),\n",
        "        # save_strategy=\"steps\",\n",
        "        # save_steps=25,\n",
        "    ),\n",
        "    data_collator=DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer, mlm=False), #모델에게 답변생성에만 집중하도록 함\n",
        "    eval_data_collator=custom_data_collator,\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "wandb_callback = CustomWandbCallback(trainer)\n",
        "trainer.add_callback(wandb_callback)\n",
        "# trainer.evaluate([train_dataset['valid'][0]])\n",
        "# model.config.rms_norm_eps = 1e-6\n",
        "trainer.train()\n",
        "\n",
        "trained_model = (trainer.model.module if hasattr(trainer.model, \"module\") else trainer.model)\n",
        "\n",
        "#로컬에 저장할 경우\n",
        "# trained_model.save_pretrained(f\"{output_dir}/saved_model\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trained_model = (trainer.model.module if hasattr(trainer.model, \"module\") else trainer.model)"
      ],
      "metadata": {
        "id": "KOy8FEIIT9w6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjKrIqGhgfKH"
      },
      "outputs": [],
      "source": [
        "# 모델 업로드\n",
        "trained_model.push_to_hub(train_name, private=True,save_embedding_layers=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoBhbLuvD47S"
      },
      "source": [
        "# Inference with Langchain (Langchain을 이용한 추론)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfhOMP5VIor2"
      },
      "outputs": [],
      "source": [
        "# gpu memory 할당 해제\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9P7OKGn-LelC"
      },
      "outputs": [],
      "source": [
        "# HuggingFacePipeline 객체 생성\n",
        "\n",
        "# 모델 ID\n",
        "if model_id is None:\n",
        "    model_id = model_cands[run_name.split('_')[0]]\n",
        "peft_model_id = train_name\n",
        "trained_model,tokenizer = load_model_w_setting(model_id)\n",
        "\n",
        "#Fine-Tune 한 LoRA 어댑터 불러오기\n",
        "trained_model.load_adapter(peft_model_id)\n",
        "\n",
        "text_generation_pipeline = pipeline(\n",
        "    model=trained_model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=200,\n",
        "    # repetition_penalty=1.5,\n",
        "    eos_token_id = tokenizer.eos_token_id,\n",
        "    pad_token_id = tokenizer.pad_token_id,\n",
        "    max_length=MAX_LEN\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nw1bpToURQU0"
      },
      "outputs": [],
      "source": [
        "#데이터셋 로드\n",
        "from datasets import load_dataset\n",
        "dataset_url = dataset_name\n",
        "dataset = load_dataset(dataset_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mF_S5nJ7-YsK"
      },
      "outputs": [],
      "source": [
        "# 검증 데이터 쓸지, 테스트 데이터 쓸지\n",
        "eval_mode = 'test' # or 'test'\n",
        "eval_dataset = dataset[eval_mode]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOexJLdyaEm8"
      },
      "outputs": [],
      "source": [
        "# 그냥 GPU 메모리 확인용\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhPtaU3BGDsS"
      },
      "outputs": [],
      "source": [
        "template.split('{answer}')[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHrZZdNsQKvp"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import trange\n",
        "\n",
        "# 결과를 저장할 리스트 초기화\n",
        "results = []\n",
        "\n",
        "# DATASET 구조를 dataset[i]={'question':,'context':,...}로 바꾸면 안됨?\n",
        "\n",
        "# Dataset 각 행에 대해 처리\n",
        "for idx in trange(len(eval_dataset['question']), desc=\"Answering Questions\"):\n",
        "    #질문, 컨텍스트(문서)\n",
        "    question = eval_dataset['question'][idx]\n",
        "    context = eval_dataset['context'][idx]\n",
        "\n",
        "    # RAG 체인 구성\n",
        "    prompt = PromptTemplate.from_template(template.split('{answer}')[0])\n",
        "\n",
        "    # RAG 체인 정의\n",
        "    if context != \"\":\n",
        "        rag_chain = (\n",
        "            RunnableParallel(context=lambda x: x[\"context\"], question = lambda x: x[\"question\"])\n",
        "            | prompt\n",
        "            | llm\n",
        "            | StrOutputParser()\n",
        "        )\n",
        "    else:\n",
        "        rag_chain = (\n",
        "            {\"question\": RunnablePassthrough()}\n",
        "            | prompt\n",
        "            | llm\n",
        "            | StrOutputParser()\n",
        "        )\n",
        "\n",
        "    # 답변 추론\n",
        "    print(f\"Question: {question}\")\n",
        "    full_response = rag_chain.invoke({\"question\": question, \"context\": context})\n",
        "\n",
        "    print(f\"Answer: {full_response}\\n\")\n",
        "\n",
        "    # 결과 저장\n",
        "\n",
        "    if context != \"\":\n",
        "        results.append({\n",
        "            \"Context\": context,\n",
        "            \"Question\": question,\n",
        "            \"Answer\": full_response,\n",
        "            \"True_Answer\": eval_dataset['answer'][idx]\n",
        "        })\n",
        "    else:\n",
        "        results.append({\n",
        "            \"Question\": question,\n",
        "            \"Answer\": full_response,\n",
        "            \"True_Answer\": eval_dataset['answer'][idx]\n",
        "        })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s04GExCGZMfG"
      },
      "outputs": [],
      "source": [
        "# i = 28\n",
        "# print_dataset_ele(eval_dataset,i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DH4MwNaVMyX"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGJfFDMzQZQW"
      },
      "outputs": [],
      "source": [
        "#검증 데이터 쓸때만 사용 가능\n",
        "if eval_mode == 'valid':\n",
        "  for item in results:\n",
        "      y_hat = item[\"Answer\"]\n",
        "      y = item[\"True_Answer\"]\n",
        "      f1, precision, recall = compute_f1(y, y_hat)\n",
        "      item[\"F1\"] = f1\n",
        "      item[\"Precision\"] = precision\n",
        "      item[\"Recall\"] = recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMd06HWVVOYX"
      },
      "outputs": [],
      "source": [
        "if eval_mode == 'valid':\n",
        "  # 제출용 샘플 파일 로드\n",
        "  eval_df = pd.DataFrame([])\n",
        "\n",
        "  # 생성된 답변을 제출 DataFrame에 추가\n",
        "  eval_df['Question'] = [item['Question'] for item in results]\n",
        "  eval_df['Answer'] = [item['Answer'] for item in results]\n",
        "  eval_df[\"F1\"] = [item[\"F1\"] for item in results]\n",
        "  eval_df[\"Precision\"] = [item[\"Precision\"] for item in results]\n",
        "  eval_df[\"Recall\"] = [item[\"Recall\"] for item in results]\n",
        "  # eval_df['Answer'] = eval_df['Answer'].fillna(\"데이콘\")     # 모델에서 빈 값 (NaN) 생성 시 채점에 오류가 날 수 있음 [ 주의 ]\n",
        "\n",
        "  save_dir = os.path.join(path,'eval')\n",
        "  if not os.path.exists(save_dir) : os.makedirs(save_dir)\n",
        "  save_name = f'eval_{fname}'\n",
        "  save_path = os.path.join(save_dir,save_name)\n",
        "\n",
        "  # 결과를 CSV 파일로 저장\n",
        "  eval_df.to_csv(save_path, encoding='UTF-8-sig', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nvAWGfY1N88"
      },
      "outputs": [],
      "source": [
        "# 평균 F1 확인\n",
        "# eval_df = pd.read_csv(f\"{path}trained_eval.csv\",index_col=0)\n",
        "if eval_mode == 'valid' :\n",
        "  display(eval_df[\"F1\"].mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ma0LX64D47T"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONaLHlk0gcg2"
      },
      "outputs": [],
      "source": [
        "submit_df = pd.read_csv(f\"{path}sample_submission.csv\")\n",
        "submit_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMrEj815D47T"
      },
      "outputs": [],
      "source": [
        "# 제출용 샘플 파일 로드\n",
        "submit_df = pd.read_csv(f\"{path}sample_submission.csv\")\n",
        "\n",
        "# 생성된 답변을 제출 DataFrame에 추가\n",
        "save_mode = 'submission'\n",
        "\n",
        "if save_mode != 'submission' :\n",
        "  submit_df['Question'] = [item['Question'] for item in results]\n",
        "  submit_df['Context'] = [item['Context'] for item in results]\n",
        "  save_dir = os.path.join(path,'eval')\n",
        "else : save_dir = os.path.join(path,'sub')\n",
        "\n",
        "if not os.path.exists(save_dir) : os.makedirs(save_dir)\n",
        "save_path = os.path.join(save_dir,fname)\n",
        "\n",
        "submit_df['Answer'] = [item['Answer'] for item in results]\n",
        "submit_df['Answer'] = submit_df['Answer'].fillna(\"데이콘\").apply(str.rstrip)     # 모델에서 빈 값 (NaN) 생성 시 채점에 오류가 날 수 있음 [ 주의 ]\n",
        "\n",
        "# 결과를 CSV 파일로 저장\n",
        "submit_df.to_csv(save_path, encoding='UTF-8-sig', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submit_df['Answer'] = submit_df['Answer'].apply(lambda x: x.split('<|eot_id|>')[0])\n",
        "submit_df.to_csv(save_path.split('.csv')[0]+'_cleaned.csv', encoding='UTF-8-sig', index=False)"
      ],
      "metadata": {
        "id": "Aj6vuHm3o6i6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ojxAcDlcpPLj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "AFuh6k7hD47P"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}