{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-CUvE2NhDog"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ShDDV9UGJjJ"
      },
      "outputs": [],
      "source": [
        "#path = '/content/drive/MyDrive/DACON/Finance/reprocessed/'\n",
        "path ='/content/drive/MyDrive/kdt-EST-AI/project/dacon_fis/src/'\n",
        "base_dir = path # Your Base Directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHsNBioph_AG"
      },
      "source": [
        "# 설명"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFuh6k7hD47P"
      },
      "source": [
        "## Question - Answering with Retrieval\n",
        "\n",
        "본 대회의 과제는 중앙정부 재정 정보에 대한 **검색 기능**을 개선하고 활용도를 높이는 질의응답 알고리즘을 개발하는 것입니다. <br>이를 통해 방대한 재정 데이터를 일반 국민과 전문가 모두가 쉽게 접근하고 활용할 수 있도록 하는 것이 목표입니다. <br><br>\n",
        "베이스라인에서는 평가 데이터셋만을 활용하여 source pdf 마다 Vector DB를 구축한 뒤 langchain 라이브러리와 llama-2-ko-7b 모델을 사용하여 RAG 프로세스를 통해 추론하는 과정을 담고 있습니다. <br>( train_set을 활용한 훈련 과정은 포함하지 않으며, test_set  에 대한 추론만 진행합니다. )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8S8yYYqpiRIj"
      },
      "source": [
        "## Mount/Login\n",
        "\n",
        "구글 드라이브를 마운트하고 허깅페이스에 로그인\n",
        "- 이때 허깅페이스 토큰은 kdt3 그룹에 대해 읽기/쓰기 권한이 있는 토큰이어야 함\n",
        "\n",
        "## Download Library\n",
        "필요/사용 라이브러리 다운로드\n",
        "이때 버전 문제로 설치를 한 뒤 세션을 한번 재시작해줘야 합니다\n",
        "<br>(그리고 세션 완전히 끊기면 다운로드 후 재시작을 다시 해줘야...)\n",
        "\n",
        "## Import Library\n",
        "한번 재시작했으면 위 과정 없이 Import만 실행해주면 됩니다\n",
        "\n",
        "## Vector DB\n",
        "문서를 여러 조각(chunk)로 나누고, 임베딩 유사도를 통해 관련 조각을 찾을 수 있게 DB화하는 함수들이 정의되어 있습니다.\n",
        "\n",
        "## DB 생성\n",
        "Vector DB에서 정의된 함수들로 문서 DB를 만들어줍니다.<br><br>\n",
        "이때 Train과 Test를 한번에 하려고 하면 코랩이 터질 확률이 높으므로 Train하고 Create Dataset까지 실행해 업로드 한 뒤 재시작해서 램을 비우고 Test를 하는 것이 좋습니다.<br> 또한 문서 임베딩을 어떤 모델로 할지 인자로 넘겨줄 수 있습니다\n",
        "\n",
        "## Create Dataset\n",
        "DB 생성에서 만든 db와 데이터 dataframe을 사용해 HuggingFace 데이터셋 생성 후 업로드\n",
        "\n",
        "## Fine-Tuning\n",
        "학습 데이터셋으로 모델에 대한 파인튜닝 진행 후 Huggingface에 업로드<br>\n",
        "4비트 양자화 LoRA로 파인튜닝<br>\n",
        "기반 모델 또는 넣어줄때 사용할 프롬프트, 학습 관련 하이퍼파라미터 수정 가능\n",
        "\n",
        "## Langchain 을 이용한 추론\n",
        "모델을 사용한 추론\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OoRmTKDoEEg"
      },
      "source": [
        "## 실행\n",
        "### 기본\n",
        "Mount/Login -> Download Library -> 재시작 (처음 1번)\n",
        "Mount/Login -> Import Library (이후)\n",
        "\n",
        "### 데이터셋 만들기\n",
        "기본 -> Vector DB -> DB 생성 -> Create Dataset에서 첫 셀 + Train/Valid/Test 중 해당하는 셀\n",
        "\n",
        "### 모델 학습하기\n",
        "기본 -> Fine-Tuning(업로드할 위치, 데이터셋 위치, 모델 링크 확인 필수)\n",
        "\n",
        "### 학습된 모델로 추론하기\n",
        "기본 -> Langchain을 이용한 추론(모델 링크, 데이터셋 위치 확인) -> Submission(저장할 파일명 확인)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_CQb3rOiCHi"
      },
      "source": [
        "# Mount/Login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jh2NMgNqp8Fp"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fDG2OcGQdOr"
      },
      "outputs": [],
      "source": [
        "ls {path}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rB6wS4zCH4YJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "token_path = os.path.join(base_dir,'data','token')\n",
        "with open(token_path,'r') as f:\n",
        "    master_token = f.readline().strip('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-ffQqQjdu0d"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=master_token, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTTusGUGD47Q"
      },
      "source": [
        "# Download Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fo2XA53bD47Q"
      },
      "outputs": [],
      "source": [
        "!apt-get install tesseract-ocr\n",
        "!apt-get install poppler-utils\n",
        "\n",
        "!pip install orjson==3.10.6\n",
        "\n",
        "!pip install accelerate\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes\n",
        "!pip install transformers[torch] -U\n",
        "\n",
        "!pip install datasets\n",
        "!pip install langchain\n",
        "!pip install langchain_community\n",
        "!pip install langchain-teddynote\n",
        "!pip install PyMuPDF\n",
        "!pip install sentence-transformers\n",
        "!pip install faiss-gpu\n",
        "!pip install unstructured pdfminer.six\n",
        "!pip install pillow-heif\n",
        "!pip install pikepdf pypdf\n",
        "\n",
        "!pip install pymupdf4llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CB-3QyUXD47R"
      },
      "source": [
        "# Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiF6VIEmD47R"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import unicodedata\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "from langchain.document_loaders.parsers.pdf import PDFPlumberParser\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    pipeline,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from accelerate import Accelerator\n",
        "\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
        "\n",
        "# PDF 로딩/청크화 관련\n",
        "from langchain.document_loaders.parsers.pdf import PDFPlumberParser\n",
        "from langchain.document_loaders.pdf import PDFPlumberLoader\n",
        "from langchain.document_loaders import UnstructuredPDFLoader\n",
        "from langchain_teddynote.retrievers import KiwiBM25Retriever\n",
        "from langchain.retrievers import EnsembleRetriever, MultiQueryRetriever\n",
        "\n",
        "from unstructured.cleaners.core import clean_extra_whitespace, clean, clean_non_ascii_chars\n",
        "\n",
        "import pymupdf4llm\n",
        "import pymupdf\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gpu memory 할당 해제\n",
        "import gc, time\n",
        "\n",
        "def free_cuda():\n",
        "  mem = 1\n",
        "  while mem > 0 :\n",
        "    time.sleep(0.5)\n",
        "    mem = gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"freed : \",mem)"
      ],
      "metadata": {
        "id": "S4g9VnSBR7xA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQYNmuXlD47R"
      },
      "source": [
        "# Vector DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kna3aYxBUMo2"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from unstructured.cleaners.core import clean_extra_whitespace, clean, clean_non_ascii_chars\n",
        "\n",
        "\n",
        "# 불릿포인트 제거용 함수\n",
        "def remove_bulletpoints(text):\n",
        "    cleaned_text = text\n",
        "    for symbol in ['ㅇ','-','□', '※', '▸','∙','●','☞','■','\u001a','\u0007','·']:\n",
        "        cleaned_text = cleaned_text.replace(symbol, f\"-\")\n",
        "    return cleaned_text\n",
        "\n",
        "def replace_sign_symbol(text):\n",
        "    cleaned_text = text\n",
        "    cleaned_text = cleaned_text.replace('△', \"-\")\n",
        "    return cleaned_text\n",
        "\n",
        "\n",
        "# 숫자 심볼 숫자로 변환\n",
        "def replace_num_symbols_with_number(text):\n",
        "    cleaned_text = text\n",
        "    for idx, symbol in enumerate(['①', '②', '③', '④', '⑤', '⑥', '⑦', '⑧', '⑨', '⑩', '⑪', '⑫', '⑬', '⑭', '⑮']):\n",
        "        cleaned_text = cleaned_text.replace(symbol, f\"{idx+1})\")\n",
        "    return cleaned_text\n",
        "\n",
        "def erase_unicode_chr(text):\n",
        "  return re.sub(r'\\\\u[0-9a-fA-F]{4}','-',text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_path(path):\n",
        "    \"\"\"경로 유니코드 정규화\"\"\"\n",
        "    return unicodedata.normalize('NFC', path)\n",
        "\n",
        "def process_path(base_dir,file_path):\n",
        "  norm_path = normalize_path(file_path)\n",
        "  if not os.path.isabs(norm_path):\n",
        "    return os.path.normpath(os.path.join(base_dir, norm_path))\n",
        "  else : return norm_path\n",
        "\n",
        "def subpath_list(dir_path):\n",
        "  return list(map(lambda x : os.path.join(dir_path,x),os.listdir(dir_path)))\n",
        "\n",
        "def processed_path_matcher(dir_path,file_path):\n",
        "  sub_list = subpath_list(dir_path)\n",
        "  path_list = list()\n",
        "  for sub in sub_list:\n",
        "    path_list.extend(subpath_list(sub))\n",
        "  prcssd_list =list(map(normalize_path,path_list))\n",
        "  for real_path,prcssd_path in zip(path_list,prcssd_list) :\n",
        "    if file_path == prcssd_path : return real_path\n",
        "  else : return file_path"
      ],
      "metadata": {
        "id": "54Bff7uJWguE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxRNM7NqD47S"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "import re\n",
        "\n",
        "def remove_table_spaces(text):\n",
        "  text = re.sub(r'[ \\t\\r]+',' ',text)\n",
        "  text = re.sub(r'[\\n\\v\\f]+','\\n',text)\n",
        "  text = re.sub(r'\\|:?[\\-]+:?(?=[\\|])','|-',text)\n",
        "  return text\n",
        "\n",
        "\n",
        "def clean_string(text):\n",
        "    text_string = clean(text, dashes=True,trailing_punctuation=True, bullets=True)\n",
        "    text_string = replace_num_symbols_with_number(text_string)\n",
        "    text_string = remove_bulletpoints(text_string)\n",
        "    return text_string\n",
        "\n",
        "def clean_table(text_string):\n",
        "#    text_string = remove_table_spaces(text_string)\n",
        "    text_string = replace_num_symbols_with_number(text_string)\n",
        "    text_string = replace_sign_symbol(text_string)\n",
        "    text_string = remove_bulletpoints(text_string)\n",
        "    return erase_unicode_chr(text_string)\n",
        "\n",
        "# 전체 마크다운 처리\n",
        "def process_pdf(file_path, chunk_size=256, chunk_overlap=32):\n",
        "    \"\"\"PDF 텍스트 추출 후 chunk 단위로 나누기\"\"\"\n",
        "    # PDF 파일 열기\n",
        "    doc = pymupdf4llm.to_markdown(file_path)\n",
        "\n",
        "    headers_to_split_on = [\n",
        "        (\"#\",\"Header 1\"),\n",
        "        (\"##\",\"Header 2\"),\n",
        "        (\"###\",\"Header 3\"),\n",
        "    ]\n",
        "\n",
        "    md_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
        "    md_chunks = md_splitter.split_text(doc)\n",
        "\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap\n",
        "    )\n",
        "    chunks = splitter.split_documents(md_chunks)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def create_vector_db(chunks, model_path=\"intfloat/multilingual-e5-small\"):\n",
        "    \"\"\"FAISS DB 생성\"\"\"\n",
        "    # 임베딩 모델 설정\n",
        "    model_kwargs = {'device': 'cuda'}\n",
        "    encode_kwargs = {'normalize_embeddings': True}\n",
        "    embeddings = HuggingFaceEmbeddings(\n",
        "        model_name=model_path,\n",
        "        model_kwargs=model_kwargs,\n",
        "        encode_kwargs=encode_kwargs\n",
        "    )\n",
        "    # FAISS DB 생성 및 반환\n",
        "    db = FAISS.from_documents(chunks, embedding=embeddings)\n",
        "    return db\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "def check_and_mkdir(func):\n",
        "    def wrapper(*args,**kwargs):\n",
        "        if not os.path.exists(args[0]): os.makedirs(args[0])\n",
        "        return func(*args,**kwargs)\n",
        "    return wrapper\n",
        "\n",
        "@check_and_mkdir\n",
        "def save_pkl(save_dir,file_name,save_object):\n",
        "    if not os.path.exists(save_dir): os.mkdir(save_dir)\n",
        "    file_path = os.path.join(save_dir,file_name)\n",
        "    with open(file_path,'wb') as f:\n",
        "        pickle.dump(save_object,f)\n",
        "\n",
        "def load_pkl(file_path):\n",
        "    with open(file_path,'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    return data"
      ],
      "metadata": {
        "id": "_l1X7VRpVaqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8dCJ7G5KAFQ"
      },
      "source": [
        "# Preprocessing Tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMatFDz8KeTW"
      },
      "outputs": [],
      "source": [
        "!pip install gmft\n",
        "!pip install git+https://github.com/conjuncts/gmft_pymupdf.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwUuQf6kKgRg"
      },
      "outputs": [],
      "source": [
        "import gmft.table_detection\n",
        "import gmft\n",
        "import markdown\n",
        "from gmft.auto import CroppedTable, TableDetector, AutoTableFormatter, AutoFormatConfig\n",
        "from gmft.auto import AutoTableDetector, TATRDetectorConfig\n",
        "from gmft.pdf_bindings import PyPDFium2Document\n",
        "from gmft_pymupdf import PyMuPDFPage\n",
        "from collections import defaultdict\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ipdb"
      ],
      "metadata": {
        "id": "rkmdCGFDlLix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from ipdb import set_trace\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def close_event():\n",
        "    plt.close() #timer calls this function after 3 seconds and closes the window\n",
        "\n",
        "fig = plt.figure()\n",
        "timer = fig.canvas.new_timer(interval = 3000) #creating a timer object and setting an interval of 3000 milliseconds\n",
        "timer.add_callback(close_event)"
      ],
      "metadata": {
        "id": "7fzOqPxwlJJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## functions"
      ],
      "metadata": {
        "id": "1dj2jDlxaFfw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### process info about page and box"
      ],
      "metadata": {
        "id": "zae3GOzpU6Az"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "-height\n",
        "8pt : 2.82mm , a4 : 210mm * 297mm\n",
        "ratio : 2.82/297 ~ 0.0094\n",
        "-width\n",
        "same length as height : get height and use it\n",
        "'''\n",
        "\n",
        "def bound_page(box,page):\n",
        "  x0 = min(max(box[0],page[0]),page[2])\n",
        "  x1 = min(max(box[1],page[1]),page[3])\n",
        "  x2 = max(min(box[2],page[2]),page[0])\n",
        "  x3 = max(min(box[3],page[3]),page[1])\n",
        "  return (x0,x1,x2,x3)\n",
        "\n",
        "def check_exclusv_range(ran1,ran2,ths):\n",
        "  flag1 = (ran1[0] - ran2[1] >= -ths)\n",
        "  flag2 = (ran2[0] - ran1[1] >= -ths)\n",
        "  return flag1 or flag2\n",
        "\n",
        "def check_exclusive_box(box1,box2,ths):\n",
        "  flag_horiz = check_exclusv_range((box1[0],box1[2]),(box2[0],box2[2]),ths)\n",
        "  flag_verti = check_exclusv_range((box1[1],box1[3]),(box2[1],box2[3]),ths)\n",
        "  return flag_horiz or flag_verti\n",
        "\n",
        "def union_box(box1,box2):\n",
        "  return min(box1[0],box2[0]),min(box1[1],box2[1]),max(box1[2],box2[2]),max(box1[3],box2[3])\n",
        "\n",
        "def check_pairly_linked(ele_list,check_not_link,union_func):\n",
        "  elements = copy.deepcopy(ele_list)\n",
        "  for i0,e0 in enumerate(elements):\n",
        "    for i1,e1 in enumerate(elements):\n",
        "      if i0 >= i1 : continue\n",
        "      if not check_not_link(e0,e1):\n",
        "        elements.pop(i1)\n",
        "        elements.pop(i0)\n",
        "        new = union_func(e0,e1)\n",
        "        elements.append(new)\n",
        "    #    print(i0,i1,new,len(elements))\n",
        "    #  else : print('=',i0,i1)\n",
        "  return elements\n",
        "\n",
        "import copy\n",
        "\n",
        "def union_pairly_linked(ele_list,check_not_link,union_func):\n",
        "  elements = ele_list\n",
        "  cnt,bnd = 0,2**len(ele_list)\n",
        "  while True :\n",
        "    #print('-'*5,cnt,'-'*5)\n",
        "    rslt = check_pairly_linked(elements,check_not_link,union_func)\n",
        "    if len(rslt) == len(elements) or len(rslt) < 2 : break\n",
        "    if cnt > bnd : break\n",
        "    elements,cnt = rslt, cnt+1\n",
        "  return rslt\n",
        "\n",
        "def get_ths(page,ths_ratio=0.0094):\n",
        "  return ths_ratio * (page[3]-page[1])\n",
        "\n",
        "def organize_box(box_list,page,ths):\n",
        "  box_list = list(map(lambda x : bound_page(x,page),box_list))\n",
        "  check_exclusv = lambda x,y : check_exclusive_box(x,y,ths)\n",
        "  return union_pairly_linked(box_list,check_exclusv,union_box)\n",
        "\n",
        "def get_bbox(tables):\n",
        "  return list(map(lambda x : x.bbox,list(tables)))\n",
        "\n",
        "def get_page_size(page,ths=0):\n",
        "  area = (0,0,page[2]-page[0],page[3]-page[1])\n",
        "  return expand_bbox_by_ths(area,area,ths)\n",
        "\n",
        "def expand_bbox_by_ths(bbox,area,ths=0):\n",
        "  bbox =(bbox[0]-ths,bbox[1]-ths,bbox[2]+ths,bbox[3]+ths)\n",
        "  return bound_page(bbox,area)\n",
        "\n",
        "def larger_v_ths(area,ths=0):\n",
        "  size = get_page_size(area)\n",
        "  return (size[2]>=ths) and (size[3]>=ths)\n",
        "\n",
        "def infer_bbox_pos(area,bbox):\n",
        "  return bbox[0]+area[0],bbox[1]+area[1],bbox[2]+area[0],bbox[3]+area[1]"
      ],
      "metadata": {
        "id": "p3c3vjSUWzFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### process detector,formatter,tables"
      ],
      "metadata": {
        "id": "XWyf14H9VJ6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gmft.auto import AutoTableDetector\n",
        "from gmft_pymupdf import PyMuPDFPage\n",
        "\n",
        "\n",
        "def get_ft_bbox(ft,area,ths=0):\n",
        "#  return ft.rect.bbox\n",
        "  rslt = organize_box(ft.fctn_results['boxes'],area,ths)[0]\n",
        "  return infer_bbox_pos(ft.rect.bbox,rslt) #expand_bbox_by_ths(rslt,area,ths)\n",
        "\n",
        "def define_formatter():\n",
        "    config = AutoFormatConfig()\n",
        "    config.semantic_spanning_cells=True\n",
        "    config.semantic_hierarchical_left_fill='deep'\n",
        "    config.enable_multi_header=False\n",
        "    config.torch_device= device\n",
        "    config.total_overlap_reject_threshold = 0.3\n",
        "    config.large_table_assumption = True\n",
        "    config.verbose = 2\n",
        "    formatter = AutoTableFormatter(config=config)\n",
        "    return formatter\n",
        "\n",
        "def define_detector():\n",
        "    config =TATRDetectorConfig()\n",
        "    config.torch_device= device\n",
        "    config.detector_base_threshold=0.75\n",
        "#    config.detector_base_threshold=0.6\n",
        "    detector = AutoTableDetector(config=config)\n",
        "    return detector\n",
        "import re\n",
        "\n",
        "def erase_constant_rowcol(df,val):\n",
        "  cols = range(len(df.columns))\n",
        "  df_temp = df.set_axis(cols,axis=1)\n",
        "  cond = df_temp == val\n",
        "  target = list(filter(lambda col : np.sum(cond[col]) != len(cond),cols))\n",
        "  df_temp, cond = df_temp[target], cond[target]\n",
        "  cond2 = np.sum(cond,axis=1) != len(target)\n",
        "  return df_temp[cond2]\n",
        "\n",
        "def maybe_numeric_table(df,ths=0.35,line_ths=0.9):\n",
        "  df_temp = df.fillna('').astype(str)\n",
        "  df_temp = erase_constant_rowcol(df_temp,'0')\n",
        "  #df_temp = df.replace(r'(?:(\\d+?)),(\\d+?)',r'\\1\\2',regex=True)\n",
        "  df_temp = df.replace(r'[\\,\\(\\)\\-\\+\\.\\s]','',regex=True)\n",
        "  df_temp = df_temp[list(df_temp.columns)].apply(pd.to_numeric,errors='coerce')\n",
        "  rslt = ~df_temp.isna()\n",
        "  rowwise = rslt.apply(sum,axis=1)\n",
        "  colwise = rslt.apply(sum,axis=0)\n",
        "  if np.sum(rowwise * line_ths <= len(rslt)) > 0 : return True\n",
        "  if np.sum(colwise * line_ths <= len(rslt.columns)) > 0 : return True\n",
        "  if np.sum(rslt.values) > len(rslt)*len(rslt.columns)*ths : return True\n",
        "  return False\n",
        "\n",
        "def check_table_df_soundness(df,ths=0.5):\n",
        "  if len(df) < 1 : return False\n",
        "  df_temp = df.replace(to_replace=[None], value=0).astype(str)\n",
        "  if maybe_numeric_table(df_temp) : return True\n",
        "  df_temp = df.replace(to_replace=[None], value='PD_NONE')\n",
        "  cols = list(df.columns.astype(np.string_))\n",
        "  cols = list(map(lambda x : '' if x is None else str(x),cols))\n",
        "  cols = list(map(lambda x : re.sub(r'[\\s]*','',x),cols))\n",
        "  null_col = list(filter(lambda x: len(x)<1,cols))\n",
        "  if len(null_col) > len(cols) * ths : return False\n",
        "  if np.sum(df_temp=='PD_NONE') > len(df)*len(cols)*ths : return False\n",
        "  return True\n",
        "\n",
        "def detect_sound_table(dt):\n",
        "  if len(dt.text()) > 2 : return True\n",
        "  return False\n",
        "\n",
        "def find_tables(page,area,ths=0):\n",
        "  detector,formatter = define_detector(),define_formatter()\n",
        "  doc = PyMuPDFPage(page)\n",
        "  dt_whole = detector.extract(doc)\n",
        "  dt_whole = list(filter(detect_sound_table,dt_whole))\n",
        "  gmft_bboxes = list(map(lambda x : x.bbox,dt_whole))\n",
        "  searched = get_bbox(page.find_tables())\n",
        "  searched.extend(gmft_bboxes)\n",
        "  searched = organize_box(searched,area,ths)\n",
        "  searched = list(map(lambda x: expand_bbox_by_ths(x,area,ths),searched))\n",
        "  rslt = list(map(lambda x:make_table(x,doc,area,formatter,ths),searched))\n",
        "  return list(filter(lambda x : x is not None,rslt))\n",
        "\n",
        "def make_table(bbox,doc,area,formatter,ths=0):\n",
        "  rect = gmft.common.Rect(bbox)\n",
        "  temp = gmft.table_detection.CroppedTable(doc,rect,0.8) #confidence level 조정이 표 인식에 영향 있을지도\n",
        "  ft = formatter.extract(temp)\n",
        "  #display(ft.rect.bbox,ft.visualize())\n",
        "  try :\n",
        "    tab_box = get_ft_bbox(ft,area,ths)\n",
        "    caption = '\\t'.join(ft.captions()) if 'captions' in ft.__dir__() else ''\n",
        "    df_tab = ft.df()\n",
        "    if not check_table_df_soundness(df_tab) :\n",
        "      print('table does not sound')\n",
        "      raise Exception('table does not sound')\n",
        "    #else : print('table sounds')\n",
        "    table = {'content':df_tab,'bbox':tab_box,'caption':caption} #, 'ft':ft}\n",
        "    return table\n",
        "  except Exception as e:\n",
        "    try :\n",
        "      display(df_tab)\n",
        "      df_tab.msg = e\n",
        "      table = df_tab\n",
        "    except : table = None\n",
        "    display(ft.visualize())\n",
        "    print('exception : ',e)\n",
        "    return table"
      ],
      "metadata": {
        "id": "9cZs0O2bW2qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### search in page"
      ],
      "metadata": {
        "id": "PY9fCv2HVV2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "if table found, search left and right\n",
        "and then, the area will be colored\n",
        "'''\n",
        "\n",
        "def get_area(area,page,left):\n",
        "  if left : return page[0],area[1],area[0],area[3]\n",
        "  else : return area[2],area[1],page[2],area[3]\n",
        "\n",
        "\n",
        "def get_blank(searched_row,page,ths=0):\n",
        "  rslt,point = list(), page[1]\n",
        "  searched_row = sorted(searched_row,key=lambda x: x[1])\n",
        "  searched_row.append((page[0],page[3],page[2],page[3]))\n",
        "  for row in searched_row[:-1]:\n",
        "    if row[1] - point < ths : continue\n",
        "    rslt.append((page[0],point,page[2],row[1]))\n",
        "    point=row[3]\n",
        "  return [bound_page(box,page) for box in rslt]\n",
        "\n",
        "\n",
        "def extend_list_dict(a,b):\n",
        "  rslt = defaultdict(list)\n",
        "  for key,val in a.items():\n",
        "    rslt[key].extend(val)\n",
        "  for key,val in b.items():\n",
        "    rslt[key].extend(val)\n",
        "  return rslt\n",
        "\n",
        "def search_page(page,area,ths=0,depth=0):\n",
        "  if not larger_v_ths(area,ths) : return defaultdict(list)\n",
        "  if depth >=10 : raise Exception(depth)\n",
        "#  print('depth : ',depth,'area : ',area)\n",
        "  page.set_cropbox(area) #area : page에서 절대적 위치. cropbox를 하게 되면 상대적 위치로 바뀜\n",
        "  rslt,searched = defaultdict(list),list()\n",
        "#  set_trace()\n",
        "  detected = find_tables(page,get_page_size(area),ths)\n",
        "  if len(detected)==0 : return rslt\n",
        "  for target in detected:\n",
        "    if target is None : continue\n",
        "    if type(target) is not dict : rslt['errs'].append(target)\n",
        "    rect = infer_bbox_pos(area,target['bbox'])\n",
        "    bbox = {'bbox':rect,'depth':depth}\n",
        "    rslt['tabs'].append(bbox)\n",
        "    print('detected:',rect,'\\t at',area,f' in depth {depth}')\n",
        "\n",
        "    left_area = get_area(rect,area,True)\n",
        "    right_area = get_area(rect,area,False)\n",
        "    if depth > 5 : print(f'left {left_area}\\tright{right_area}')\n",
        "    left = search_page(page,left_area,ths,depth+1)\n",
        "    right = search_page(page,right_area,ths,depth+1)\n",
        "    searched.append((area[0],rect[1],area[2],rect[3]))\n",
        "    if depth > 5 : print(searched)\n",
        "    extend_list_dict(rslt,left)\n",
        "    extend_list_dict(rslt,right)\n",
        "\n",
        "  if len(rslt)==0 : return rslt\n",
        "  searched = organize_box(searched,area,ths)\n",
        "  blanks = get_blank(searched,area,ths)\n",
        "  if depth >5 : print(f'in {area}\\n\\t',blanks)\n",
        "  for row in blanks:\n",
        "    detected = search_page(page,row,ths,depth+1)\n",
        "    extend_list_dict(rslt,detected)\n",
        "\n",
        "#  rslt= organize_box(rslt,area,ths) : can't apply directly like this\n",
        "#  print('in search page, err : ',len(rslt['errs']))\n",
        "  page.set_cropbox(area)\n",
        "  return rslt"
      ],
      "metadata": {
        "id": "E2001sT8jF-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### extract tables and reform pdfs"
      ],
      "metadata": {
        "id": "aEoM7bIgZ4eT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_area_to_mark(mu_page,area,mark):\n",
        "    mu_page.add_redact_annot(area)\n",
        "    mu_page.apply_redactions()\n",
        "    mu_page.draw_rect(area,color=(.0,0,0),fill=(.99,.99,.99))\n",
        "    rc = mu_page.insert_htmlbox(area,mark,scale_low=0)\n",
        "    return mu_page\n",
        "\n",
        "def extract_tables_from_pdf(full_path,tab_word='[[TABLE_{0}]]'):\n",
        "    pdf = pymupdf.open(full_path)\n",
        "    chunks, tables_dict, cnt= list(), defaultdict(list),0\n",
        "    err_dict = dict()\n",
        "    if pdf is None : return None, tables_dict\n",
        "    for pnum, page in enumerate(tqdm(pdf)):\n",
        "      page_area = tuple(page.mediabox)\n",
        "      try:\n",
        "        tab_rslt = search_page(page,page_area,get_ths(page_area))\n",
        "        bboxes, errors = tab_rslt['tabs'],tab_rslt['errs']\n",
        "#        print('errcheck ',len(errors))\n",
        "        tables,doc =list(), PyMuPDFPage(page)\n",
        "        formatter = define_formatter()\n",
        "        for box in bboxes:\n",
        "          table = make_table(box['bbox'],doc,page_area,formatter,get_ths(page_area))\n",
        "          if table is None :\n",
        "            print('error : ',box['bbox'])\n",
        "            continue\n",
        "          if type(table) is not dict : errors.append(table)\n",
        "          else :\n",
        "            table['depth'] = box['depth']\n",
        "            tables.append(table)\n",
        "\n",
        "      except:\n",
        "        save_pkl(ERRORDIR,'page_doc_error,pkl',[full_path,pnum,page_area])\n",
        "        raise Exception()\n",
        "      if len(errors)>0 :\n",
        "        err_dict[pnum] = errors\n",
        "        print('errs : ',len(errors))\n",
        "      if len(tables) == 0 : continue\n",
        "      print(f'detected in p.{pnum} :\\t',len(tables),' tables')\n",
        "      tables = sorted(tables,key=lambda x : (x['bbox'][0],x['bbox'][1]))\n",
        "      for idx,tab in enumerate(tables):\n",
        "        tab_mark = tab_word.format(cnt+idx)\n",
        "        table_md = clean_table(tab['content'].to_markdown(index=False))\n",
        "        tables_dict[pnum].append((tab_mark,table_md + f\"\\n{tab['caption']}\"))\n",
        "\n",
        "        try :\n",
        "          area = tab['bbox']\n",
        "          page.add_redact_annot(area)\n",
        "          page.apply_redactions()\n",
        "          page.draw_rect(area,color=(.0,0,0),fill=(.99,.99,.99))\n",
        "          rc = page.insert_htmlbox(area,tab_mark,scale_low=0)\n",
        "        except :\n",
        "          print(page.mediabox)\n",
        "          print(page.cropbox)\n",
        "          print(tab['bbox'])\n",
        "          display(tab['content'])\n",
        "          print(table_md)\n",
        "\n",
        "      cnt+=len(tables)\n",
        "\n",
        "    print(cnt, len(tables_dict), sum([len(tables) for tables in tables_dict.values()]))\n",
        "    return pdf, tables_dict,err_dict\n",
        "\n",
        "def extract_table_and_pdf(pdf_path,base_path,save_dir):\n",
        "    # 경로 정규화 및 절대 경로 생성\n",
        "    norm_path = normalize_path(pdf_path)\n",
        "    full_path = process_path(base_path,pdf_path)\n",
        "    full_path = processed_path_matcher(base_path,full_path)\n",
        "    pdf_name = os.path.basename(full_path)\n",
        "\n",
        "    print(f\"Processing {pdf_name}...\")\n",
        "    save_path = os.path.join(save_dir, norm_path)\n",
        "    print(full_path,save_path)\n",
        "    pdf_dir = os.path.dirname(save_path)\n",
        "    if not os.path.exists(pdf_dir) : os.makedirs(pdf_dir)\n",
        "    new_pdf,tab_list,err_list = extract_tables_from_pdf(full_path,tab_word)\n",
        "    save_pkl(pdf_dir,pdf_name[:-4]+'.pkl',tab_list)\n",
        "    save_pkl(pdf_dir,'err_'+pdf_name[:-4]+'.pkl',err_list)\n",
        "    new_pdf.save(save_path,garbage=4,deflate=True)\n",
        "    return tab_list,err_list\n",
        "\n",
        "def reform_pdfs_from_df(df, base_path,save_dir,name='data'):\n",
        "    \"\"\"딕셔너리에 pdf명을 키로해서 DB, retriever 저장\"\"\"\n",
        "    unique_paths = df['Source_path'].unique()\n",
        "    tab_dict,err_dict = dict(), dict()\n",
        "    for path in tqdm(unique_paths, desc=\"Processing PDFs\"):\n",
        "      print(path,base_path)\n",
        "      tab_dict[path],err_dict[path]=extract_table_and_pdf(path,base_path,save_dir)\n",
        "    save_pkl(os.path.join(save_dir,'tables'),f'tab_{name}.pkl',tab_dict)\n",
        "    return tab_dict,err_dict"
      ],
      "metadata": {
        "id": "zvn5s2nDXJ-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## read table marks"
      ],
      "metadata": {
        "id": "YrBnGu-NaMo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_neg_idx(idxs,len_obj):\n",
        "  rslt = deepcopy(idxs)\n",
        "  for i in idxs:\n",
        "    if i >= 0 : continue\n",
        "    new_num = i + len_obj\n",
        "    del rslt[i]\n",
        "    rslt.append(new_num)\n",
        "  return rslt\n",
        "\n",
        "def add_escape(sent):\n",
        "  idxs = list(filter(lambda x : sent[x] in ['[',']'],range(len(sent))))\n",
        "  temp, idxs = list(sent), convert_neg_idx(idxs,len(sent))\n",
        "  idxs = sorted(idxs)[::-1]\n",
        "  for i in idxs:\n",
        "    temp.insert(i,'\\\\')\n",
        "  return ''.join(temp)"
      ],
      "metadata": {
        "id": "ueOLqkdh0KiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from copy import deepcopy\n",
        "from collections import defaultdict\n",
        "from langchain_core.documents import Document as Doc\n",
        "\n",
        "def get_former_idx(err_list):\n",
        "  rslt = list()\n",
        "  for i in err_list:\n",
        "    cand = list(filter(lambda x : x not in err_list,range(i)))\n",
        "    idx = max(cand) if cand else 0\n",
        "    rslt.append(idx)\n",
        "  return rslt\n",
        "\n",
        "def get_latter_idx(err_list):\n",
        "  rslt = list()\n",
        "  for i in err_list:\n",
        "    cand = list(filter(lambda x : x not in err_list,range(i,err_list[-1]+2)))\n",
        "    idx = min(cand) if cand else err_list[-1]+1\n",
        "    rslt.append(idx)\n",
        "  return rslt\n",
        "\n",
        "def make_table_page(content,tab_mark,table,tab_caption=None,th_len=100):\n",
        "  if len(content)<len(tab_mark) : return None, None\n",
        "  if tab_caption is None : tab_caption = tab_mark\n",
        "  re_sep = '[\\s\\|]*'\n",
        "  re_mark = insert_btwn_chr(tab_mark,re_sep)\n",
        "  re_trgt = re.compile(re_mark)\n",
        "  flag = list(re.finditer(re_trgt,content))\n",
        "  if flag :\n",
        "      front,end = flag[0].pos,flag[0].endpos\n",
        "      start = min(0,front-th_len)\n",
        "      new_page = content[start:front] + '\\n' + table + f'\\n{tab_caption}'\n",
        "      #page = content[:front]+tab_caption+content[end:]\n",
        "      page = content\n",
        "  else : new_page, page = None, None\n",
        "  return new_page,page\n",
        "\n",
        "def get_insert_idx(former_idx,latter_idx,tab_page):\n",
        "  rslt = list()\n",
        "  for former,latter in zip(former_idx,latter_idx):\n",
        "    pages = tab_page.values()\n",
        "    first, last = 0,max(pages)\n",
        "    i0 = tab_page[former] if former in tab_page else first\n",
        "    i1 = tab_page[latter] if latter in tab_page else last\n",
        "    rslt.append(int((i0+i1)/2))\n",
        "  return rslt\n",
        "\n",
        "def set_err_tab_page(new_pages,err_list,table_list,tab_page,tab_caption):\n",
        "  if len(err_list) == 0 : return new_pages\n",
        "  err_idx, err_tabs = zip(*err_list)\n",
        "  if len(new_pages) == 0 : insert_idx = [-1 for _ in err_list]\n",
        "  else :\n",
        "    former_idx = get_former_idx(err_idx)\n",
        "    latter_idx = get_latter_idx(err_idx)\n",
        "    insert_idx = get_insert_idx(former_idx,latter_idx,tab_page)\n",
        "  for page,i_tab,tab in zip(insert_idx,err_idx,err_tabs):\n",
        "    content =tab +'\\n'+ tab_caption.format(i_tab)\n",
        "    new_pages[page].append(Doc(page_content=content))\n",
        "  return new_pages\n",
        "\n",
        "def convert_neg_num_page(page_dict,book_len):\n",
        "  rslt = deepcopy(page_dict)\n",
        "  for page,docs in page_dict.items():\n",
        "    if page >= 0 : continue\n",
        "    new_num = page+book_len\n",
        "    del rslt[page]\n",
        "    rslt[new_num] = docs\n",
        "  return rslt\n",
        "\n",
        "def insert_pages(doc_list,new_pages):\n",
        "  new_pages = convert_neg_num_page(new_pages,len(doc_list))\n",
        "  page_list = sorted(list(new_pages.keys()))[::-1]\n",
        "  for page in page_list:\n",
        "    if page >= len(doc_list) -1 : doc_list += new_pages[page]\n",
        "    else : doc_list = doc_list[:page+1]+new_pages[page]+doc_list[page+1:]\n",
        "  return doc_list\n",
        "\n",
        "def get_table_page(num,doc_list,tab_mark,table):\n",
        "    this_page = doc_list[num]['text']\n",
        "    next_page = doc_list[num+1]['text'] if num+1 < len(doc_list) else ''\n",
        "    both_page = this_page + next_page if next_page != '' else ''\n",
        "\n",
        "    this_rslt,page0 = make_table_page(this_page,tab_mark,table)\n",
        "    next_rslt,page1 = make_table_page(next_page,tab_mark,table)\n",
        "    both_rslt,page2 = make_table_page(both_page,tab_mark,table)\n",
        "\n",
        "    if this_rslt is not None : page_content,page = this_rslt, page0\n",
        "    if next_rslt is not None : page_content,page,num = next_rslt, page1,num+1\n",
        "    elif both_rslt is not None :\n",
        "      page_content,page = both_rslt, page2[:len(this_page)+abs(len(both_page)-len(both_rslt))]\n",
        "    else : page_content,page = None, this_page\n",
        "    return page_content, page, num\n",
        "\n",
        "def expand_pages(doc_list,new_pages):\n",
        "  for page,tables in new_pages.items():\n",
        "    content = doc_list[page]['text']+'\\n'+'\\n'.join(tables)\n",
        "    doc_list[page]['text'] = content\n",
        "  return doc_list\n",
        "\n",
        "def insert_table_2_doc(doc_list,table_dict,tab_word='[[TABLE_{0}]]'):\n",
        "  new_pages,cnt = defaultdict(list),0\n",
        "  for num,table_info in table_dict.items():\n",
        "    for tab_mark,table in table_info:\n",
        "      table_page,page_adjst,page_num = get_table_page(num,doc_list,tab_mark,table)\n",
        "      if table_page is not None:\n",
        "        new_pages[page_num].append(table_page)\n",
        "#        doc_list[page_num]['text'] = page_adjst\n",
        "        cnt+=1\n",
        "      else : new_pages[num].append(table+'\\n'+tab_mark)\n",
        "\n",
        "  doc_list = expand_pages(doc_list,new_pages)\n",
        "  return doc_list, cnt/sum(map(len,table_dict.values()))\n",
        "\n",
        "#def insert_table_2_doc(doc_list,table_list,tab_word='[[TABLE_{0}]]'):\n",
        "#  cnt = 0\n",
        "#  tab_list=[Doc(page_content = '#별첨\\n본문에 첨부되어 있던 표')]\n",
        "#  for i,table in enumerate(table_list):\n",
        "#    tab_mark = tab_word.format(i)\n",
        "#    for num,doc in enumerate(doc_list):\n",
        "#      table_page,page_adjst,page_num = get_table_page(num,doc_list,tab_mark,table)\n",
        "#      if table_page is not None:\n",
        "#        tab_list.append(Doc(page_content = table_page))\n",
        "#        doc_list[page_num] = Doc(page_content=page_adjst,metadata=doc.metadata)\n",
        "#        cnt+=1\n",
        "#        break\n",
        "#    else : tab_list.append(Doc(page_content = table))\n",
        "#\n",
        "#  doc_list = doc_list+tab_list\n",
        "#  return doc_list, cnt/len(table_list)\n",
        "\n",
        "\n",
        "#def insert_table_2_doc(doc_list,table_list,tab_word='[[TABLE_{0}]]'):\n",
        "#  err_list,tab_page=[],dict()\n",
        "#  new_pages = defaultdict(list)\n",
        "#  for i,table in enumerate(table_list):\n",
        "#    tab_mark = tab_word.format(i)\n",
        "#    for num,doc in enumerate(doc_list):\n",
        "#      table_page,page_adjst,page_num = get_table_page(num,doc_list,tab_mark,table)\n",
        "#      if table_page is not None:\n",
        "#        new_pages[page_num+1].append(Doc(page_content = table_page, metadata=doc.metadata))\n",
        "#        tab_page[i] = page_num\n",
        "#        tab_page[i],doc_list[page_num] = page_num, Doc(page_content=page_adjst,metadata=doc.metadata)\n",
        "#        break\n",
        "#    else : err_list.append([i,table])\n",
        "#\n",
        "#  new_pages = set_err_tab_page(new_pages,err_list,table_list,tab_page,tab_word)\n",
        "#  return doc_list, 1-len(err_list)/len(table_list)\n",
        "\n",
        "def insert_btwn_chr(sent,sep):\n",
        "  c = list(add_escape(sent))\n",
        "  d = c.copy()\n",
        "  diff = (len(c)-len(sent))//2\n",
        "  for i in range(2*diff+1,len(c)-diff*2+2)[::-1]:\n",
        "    d.insert(i-1,sep)\n",
        "  return ''.join(d)\n",
        "\n"
      ],
      "metadata": {
        "id": "XeUWe1PxvX_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import difflib\n",
        "\n",
        "def union_strs(str0,str1):\n",
        "    output_list = difflib.ndiff(str0, str1)\n",
        "    return ''.join(map(lambda x : x[-1],output_list))\n",
        "\n",
        "def pdf_2_chunck_w_table(file_path, tables, tab_word,chunk_size=256, chunk_overlap=32):\n",
        "    \"\"\"PDF 텍스트 추출 후 chunk 단위로 나누기\"\"\"\n",
        "    # PDF 파일 열기\n",
        "    doc = pymupdf4llm.to_markdown(file_path,page_chunks=True,table_strategy='lines')\n",
        "    doc,rate = insert_table_2_doc(doc,tables,tab_word)\n",
        "    doc0 = '\\n'.join(map(lambda x : x['text'],doc))\n",
        "#    doc1 = pymupdf4llm.to_markdown(file_path)\n",
        "#    docs = union_strs(doc0,doc1)\n",
        "    md_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
        "    md_chunks = md_splitter.split_text(doc0)\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap\n",
        "    )\n",
        "    chunks = splitter.split_documents(md_chunks)\n",
        "    print(file_path)\n",
        "    print(f'table mark detect rate : {rate:.5f}')\n",
        "\n",
        "    return chunks,rate\n",
        "\n",
        "def make_chunk_dict_from_df(df, base_dir, table_dict, chunk_size=256):\n",
        "    \"\"\"딕셔너리에 pdf명을 키로해서 DB, retriever 저장\"\"\"\n",
        "    unique_paths = df['Source_path'].unique()\n",
        "    chunk_dict = dict()\n",
        "    err_tab_dict = dict()\n",
        "\n",
        "    for file_path in tqdm(unique_paths, desc=\"Processing PDFs\"):\n",
        "        # 경로 정규화 및 절대 경로 생성\n",
        "        full_path = process_path(base_dir,file_path)\n",
        "        full_path = processed_path_matcher(base_dir,full_path)\n",
        "        pdf_title = os.path.basename(full_path)\n",
        "        print(f\"Processing {pdf_title}...\")\n",
        "\n",
        "        # PDF 처리 및 벡터 DB 생성\n",
        "        chunk_dict[file_path]= pdf_2_chunck_w_table(full_path,table_dict[file_path],chunk_size)\n",
        "    return chunk_dict"
      ],
      "metadata": {
        "id": "m-U-YAgmwG0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#앙상블\n",
        "def process_pdfs_from_df(df, base_dir, table_dict, tab_word, chunk_size=256, model_path = \"intfloat/multilingual-e5-small\"):\n",
        "    \"\"\"딕셔너리에 pdf명을 키로해서 DB, retriever 저장\"\"\"\n",
        "    pdf_databases = {}\n",
        "    unique_paths = df['Source_path'].unique()\n",
        "    rate_dict=dict()\n",
        "\n",
        "    for file_path in tqdm(unique_paths, desc=\"Processing PDFs\"):\n",
        "        # 경로 정규화 및 절대 경로 생성\n",
        "        full_path = process_path(base_dir,file_path)\n",
        "        full_path = processed_path_matcher(base_dir,full_path)\n",
        "        pdf_title = os.path.basename(full_path)\n",
        "        print(f\"Processing {pdf_title}...\")\n",
        "\n",
        "        # PDF 처리 및 벡터 DB 생성\n",
        "        chunks,rate =pdf_2_chunck_w_table(full_path,table_dict[file_path],tab_word,chunk_size)\n",
        "        db = create_vector_db(chunks, model_path=model_path)\n",
        "\n",
        "        kiwi_bm25_retriever = KiwiBM25Retriever.from_documents(chunks)\n",
        "        faiss_retriever = db.as_retriever()\n",
        "        retriever = EnsembleRetriever(\n",
        "            retrievers=[kiwi_bm25_retriever, faiss_retriever],\n",
        "            weights=[0.5, 0.5],\n",
        "            search_type=\"mmr\",\n",
        "        )\n",
        "\n",
        "        # 결과 저장\n",
        "        pdf_databases[pdf_title] = {\n",
        "                'db': db,\n",
        "                'retriever': retriever\n",
        "        }\n",
        "        rate_dict[pdf_title] = rate\n",
        "    return pdf_databases, rate_dict"
      ],
      "metadata": {
        "id": "P4TTKGRszRq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## run codes"
      ],
      "metadata": {
        "id": "8Q7uabe3yJrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "headers_to_split_on = [\n",
        "    (\"#\",\"Header 1\"),\n",
        "    (\"##\",\"Header 2\"),\n",
        "    (\"###\",\"Header 3\"),\n",
        "]\n",
        "\n",
        "tab_word = '!표{0}!'"
      ],
      "metadata": {
        "id": "nkC09x_3FcJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(f'{base_dir}train.csv')\n",
        "test_df = pd.read_csv(f'{base_dir}test.csv')"
      ],
      "metadata": {
        "id": "-GxJssWp3JKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### move files to runtime"
      ],
      "metadata": {
        "id": "xhY66oIglfgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ls {base_dir}"
      ],
      "metadata": {
        "id": "B_1uqkO0lx0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_dirs = ['train_source/','test_source/']\n",
        "file_path = ' '.join([os.path.join(base_dir,sub) for sub in src_dirs])\n",
        "temp_path = '/content/src/'\n",
        "if not os.path.exists(temp_path) : os.makedirs(temp_path)"
      ],
      "metadata": {
        "id": "vIzkknxilr8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sub in src_dirs:\n",
        "  src_path = os.path.join(base_dir,sub)\n",
        "  dst_path = os.path.join(temp_path,sub)\n",
        "  if not os.path.exists(dst_path) : os.makedirs(dst_path)\n",
        "  !rsync -rvzh {src_path} {dst_path} --bwlimit 4096000000000000 --progress"
      ],
      "metadata": {
        "id": "PFjbVzhPm2Zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## caution to unicode normalize\n",
        "display('1-1 2024 주요 재정통계 1권.pdf' == normalize_path('1-1 2024 주요 재정통계 1권.pdf'))\n",
        "display('1-1 2024 주요 재정통계 1권.pdf' == '1-1 2024 주요 재정통계 1권.pdf')"
      ],
      "metadata": {
        "id": "T0YO8I8PnkLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROCESSEDDIR = os.path.join(base_dir,'processed')\n",
        "if not os.path.exists(PROCESSEDDIR) : os.makedirs(PROCESSEDDIR)\n",
        "ERRORDIR = os.path.join(PROCESSEDDIR,'ERRORS')\n",
        "if not os.path.exists(ERRORDIR) : os.makedirs(ERRORDIR)"
      ],
      "metadata": {
        "id": "Uvy-LC6cfkjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lF7Lae_ALK59"
      },
      "outputs": [],
      "source": [
        "reform_pdfs_from_df(train_df, temp_path,PROCESSEDDIR,'trn')\n",
        "reform_pdfs_from_df(test_df, temp_path,PROCESSEDDIR,'tst');"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Config"
      ],
      "metadata": {
        "id": "bB3KU-M7wNNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tab_ver = 'tab_v2.2'\n",
        "model_dict = {\n",
        "'large':\"intfloat/multilingual-e5-large\",\n",
        "'base':\"intfloat/multilingual-e5-base\",\n",
        "}\n",
        "\n",
        "if tab_ver == 'tab_v0' : file_dir = base_dir\n",
        "else : file_dir = os.path.join(base_dir,'processed',tab_ver)\n",
        "\n",
        "model_option = 'large'\n",
        "#model_option = 'base'\n",
        "model_path = model_dict[model_option]\n",
        "chunk_size = 512 #256\n",
        "split_ver = 'split.2'"
      ],
      "metadata": {
        "id": "Fte0N_orKlyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aug_type = \"AugGPT\""
      ],
      "metadata": {
        "id": "hvLk7SD5vrY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aug_type= 'AugAEDA'"
      ],
      "metadata": {
        "id": "M48KJ7oGxBSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aug_type= 'GPTOnly'"
      ],
      "metadata": {
        "id": "2GocucmMsgfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aug_type= 'NoAug'"
      ],
      "metadata": {
        "id": "EGmmjLq0gwzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db_config = {\n",
        "    'model' : model_option,\n",
        "    'tab_process' : tab_ver,\n",
        "    'aug' : aug_type,\n",
        "    'chunck_size' : chunk_size,\n",
        "    'split_ver' : split_ver\n",
        "}"
      ],
      "metadata": {
        "id": "MPEufovdIzEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db_name = \"{model}-ensemble-{tab_process}.{split_ver}-{chunck_size}\".format(**db_config)"
      ],
      "metadata": {
        "id": "9EtBBFKPUyL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split train/valid"
      ],
      "metadata": {
        "id": "BVtWhHV8DXJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = path"
      ],
      "metadata": {
        "id": "GiJGTx0RBPaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbb6w_D5aG6N"
      },
      "outputs": [],
      "source": [
        "data_df = pd.read_csv(os.path.join(base_dir,'train.csv'))\n",
        "test_df = pd.read_csv(os.path.join(base_dir,'test.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib as mpl\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "#fig,ax=plt.subplots()\n",
        "#sns.histplot(data_df['Source'],ax=ax)\n",
        "#a = len(data_df['Source'].unique())\n",
        "#ax.plot(range(a),len(data_df)*0.2*np.ones(a),color='#112155')\n",
        "#ax.plot(range(a),len(data_df)*0.225*np.ones(a),color='#115175')\n",
        "#ax.plot(range(a),len(data_df)*0.175*np.ones(a),color='#115175')\n",
        "#pass"
      ],
      "metadata": {
        "id": "5AIiBNMj9V6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def check_split_possible(dist,ratio,err_ths):\n",
        "  n_all = dist.sum()\n",
        "  if dist.iloc[-1] > n_all * (ratio+err_ths) : return False\n",
        "  cond = dist > n_all * (ratio+err_ths)\n",
        "  if dist[cond].sum() > n_all*(1-ratio+err_ths) : return False\n",
        "  else : return True\n",
        "\n",
        "def split_by_files(data:pd.DataFrame,col,test_size,random_state,err_ths=0.025):\n",
        "  if col not in data.columns : return False\n",
        "  nprnd = np.random.RandomState(random_state)\n",
        "  n_all = len(data)\n",
        "  dist = data[col].value_counts().sort_values(ascending=False)\n",
        "  if not check_split_possible(dist,test_size,err_ths): return False\n",
        "  train_cond = dist > n_all*(test_size-err_ths)\n",
        "  while dist[train_cond].sum() < n_all*(1-test_size-err_ths):\n",
        "    cand = nprnd.choice(dist[~train_cond].index,1)[0]\n",
        "    if dist[train_cond].sum()+dist.loc[cand] > n_all*(1-test_size+err_ths) : continue\n",
        "    else : train_cond = (dist.index == cand) | train_cond\n",
        "    # have to make hedge for infinite loop\n",
        "  train_files,test_files = dist[train_cond].index, dist[~train_cond].index\n",
        "  print(f'size : {dist[train_cond].sum()}, ratio : {dist[train_cond].sum()/n_all:.4f}, n_files : {len(train_files)}')\n",
        "  print(f'size : {dist[~train_cond].sum()}, ratio : {dist[~train_cond].sum()/n_all:.4f}, n_files : {len(test_files)}')\n",
        "  train_cond = data[col].isin(train_files)\n",
        "  return data[train_cond], data[~train_cond]"
      ],
      "metadata": {
        "id": "iwEsIivl7C3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "if split_ver == 'split.0' : train_df, valid_df = data_df, pd.DataFrame(columns=data_df.columns)\n",
        "elif split_ver == 'split.1' :train_df,valid_df = train_test_split(data_df,test_size=0.2,stratify=data_df.Source,random_state=801)\n",
        "elif split_ver == 'split.2' :train_df,valid_df = split_by_files(data_df,'Source',test_size=0.2,random_state=801)"
      ],
      "metadata": {
        "id": "Cky1osgBDdga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply Augmentation"
      ],
      "metadata": {
        "id": "xJj-dS7ITKbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ls {base_dir}"
      ],
      "metadata": {
        "id": "sJBS3Yf9TQlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "refine_option = False\n",
        "filter_option = False\n",
        "\n",
        "file_config = {\n",
        "    'refined' : refine_option,\n",
        "    'filter' : filter_option,\n",
        "}"
      ],
      "metadata": {
        "id": "A6oEIMfgTEhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if refine_option : aug_file,sep = 'combined_train_aug_v3.5_editted.csv','\\tab'\n",
        "else  : aug_file,sep = 'combined_train_aug_v3_editted.csv','|'\n",
        "#aug_file = 'combined_train_aug_v3.csv'\n",
        "aug_path = os.path.join(base_dir,aug_file)"
      ],
      "metadata": {
        "id": "BRMVnPwYTOqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ques_dict={\n",
        "    'NoAug' : 'Question',\n",
        "    'AugGPT' : 'Question_aug_GPT',\n",
        "    'AugAEDA' : 'AEDA_Question',\n",
        "    'GPTOnly' : 'Question_aug_GPT',\n",
        "}\n",
        "ans_dict = {\n",
        "    'NoAug' : 'Answer',\n",
        "    'AugGPT' : 'Answer',\n",
        "    'AugAEDA' : 'Answer',\n",
        "    'GPTOnly' : 'Answer',\n",
        "}"
      ],
      "metadata": {
        "id": "UijHxBXqh1oH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key_col = 'SAMPLE_ID'\n",
        "info_col = ['Source', 'Source_path']\n",
        "ques_base = ques_dict['NoAug']\n",
        "ans_base = ans_dict['NoAug']\n",
        "ques_col = ques_dict[aug_type]\n",
        "ans_col = ans_dict[aug_type]"
      ],
      "metadata": {
        "id": "zhtWfpPkei5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filter_list = ['TRAIN_451', 'TRAIN_452', 'TRAIN_453', 'TRAIN_454', 'TRAIN_455', 'TRAIN_456']"
      ],
      "metadata": {
        "id": "d9mV_Rqw3nCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aug_df = pd.read_csv(aug_path,sep=sep)\n",
        "aug_df.info()"
      ],
      "metadata": {
        "id": "y9dTTOk35xLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "train_id = train_df[key_col].values\n",
        "#print(pd.Series(filter_list).isin(train_id))\n",
        "cond = (aug_df[key_col].isin(train_id))\n",
        "if filter_option : cond = cond & (~(aug_df[key_col].isin(filter_list)))\n",
        "display(aug_df.columns), len(train_id), np.sum(cond)"
      ],
      "metadata": {
        "id": "vt4vX1Oh4u_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_list = [key_col]+info_col+[ques_base,ans_base]\n",
        "train_adjst = aug_df.loc[cond,col_list]\n",
        "train_df= train_adjst.rename(columns = {ques_col : \"Question\", ans_col : \"Answer\"})"
      ],
      "metadata": {
        "id": "nf5wsYFT6Wky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if aug_type != 'NoAug':\n",
        "  col_list = [key_col]+info_col+[ques_col,ans_col]\n",
        "  aug_train = aug_df.loc[cond,col_list]\n",
        "  aug_train = aug_train.rename(columns = {ques_col : \"Question\", ans_col : \"Answer\"})\n",
        "  if 'Only' in aug_type : train_augged=aug_train\n",
        "  else : train_augged= pd.concat([train_df,aug_train])\n",
        "  train_augged.info()"
      ],
      "metadata": {
        "id": "wpmbQSUgZAbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.info()"
      ],
      "metadata": {
        "id": "bHF5ffWzHDau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_id = valid_df[key_col].values\n",
        "cond = (aug_df[key_col].isin(valid_id))\n",
        "if filter_option : cond = cond & (~(aug_df[key_col].isin(filter_list)))\n",
        "col_list = [key_col]+info_col+[ques_base,ans_base]\n",
        "valid_adjst = aug_df.loc[cond,col_list]\n",
        "valid_df= valid_adjst.rename(columns = {ques_col : \"Question\", ans_col : \"Answer\"})"
      ],
      "metadata": {
        "id": "o3zSgMbTbZNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "free_cuda()"
      ],
      "metadata": {
        "id": "-7gZrExBjtfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKhUzLV5D47S"
      },
      "source": [
        "# DB 생성"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp_path = '/content/src/'\n",
        "file_dir, os.listdir(file_dir)"
      ],
      "metadata": {
        "id": "trcy6WQC4LLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_dirs = list(filter(lambda x : x != 'pdf_db',os.listdir(file_dir)))\n",
        "file_path = ' '.join([os.path.join(file_dir,sub) for sub in src_dirs])\n",
        "if not os.path.exists(temp_path) : os.makedirs(temp_path)"
      ],
      "metadata": {
        "id": "518fVSXG4rXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rsync -rvzh {file_path} {temp_path} --bwlimit 4096000000000000 --progress"
      ],
      "metadata": {
        "id": "Lvg8RRm54rXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use it when table ver < 2\n",
        "def process_pdf(file_path, chunk_size=256, chunk_overlap=32):\n",
        "    \"\"\"PDF 텍스트 추출 후 chunk 단위로 나누기\"\"\"\n",
        "    # PDF 파일 열기\n",
        "    doc = pymupdf4llm.to_markdown(file_path)\n",
        "\n",
        "    headers_to_split_on = [\n",
        "        (\"#\",\"Header 1\"),\n",
        "        (\"##\",\"Header 2\"),\n",
        "        (\"###\",\"Header 3\"),\n",
        "    ]\n",
        "\n",
        "    md_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
        "    md_chunks = md_splitter.split_text(doc)\n",
        "\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap\n",
        "    )\n",
        "    chunks = splitter.split_documents(md_chunks)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def create_vector_db(chunks, model_path=\"intfloat/multilingual-e5-small\"):\n",
        "    \"\"\"FAISS DB 생성\"\"\"\n",
        "    # 임베딩 모델 설정\n",
        "    model_kwargs = {'device': 'cuda'}\n",
        "    encode_kwargs = {'normalize_embeddings': True}\n",
        "    embeddings = HuggingFaceEmbeddings(\n",
        "        model_name=model_path,\n",
        "        model_kwargs=model_kwargs,\n",
        "        encode_kwargs=encode_kwargs\n",
        "    )\n",
        "    # FAISS DB 생성 및 반환\n",
        "    db = FAISS.from_documents(chunks, embedding=embeddings)\n",
        "    return db\n",
        "\n",
        "\n",
        "#앙상블\n",
        "def process_pdfs_from_dataframe(df, base_dir, chunk_size=256, model_path = \"intfloat/multilingual-e5-small\"):\n",
        "    \"\"\"딕셔너리에 pdf명을 키로해서 DB, retriever 저장\"\"\"\n",
        "    pdf_databases = {}\n",
        "    unique_paths = df['Source_path'].unique()\n",
        "\n",
        "    for file_path in tqdm(unique_paths, desc=\"Processing PDFs\"):\n",
        "        # 경로 정규화 및 절대 경로 생성\n",
        "        full_path = process_path(base_dir,file_path)\n",
        "        full_path = processed_path_matcher(base_dir,full_path)\n",
        "        pdf_title = os.path.basename(full_path)\n",
        "        print(f\"Processing {pdf_title}...\")\n",
        "\n",
        "        # PDF 처리 및 벡터 DB 생성\n",
        "        chunks = process_pdf(full_path,chunk_size)\n",
        "        db = create_vector_db(chunks, model_path=model_path)\n",
        "\n",
        "        kiwi_bm25_retriever = KiwiBM25Retriever.from_documents(chunks)\n",
        "        faiss_retriever = db.as_retriever()\n",
        "        retriever = EnsembleRetriever(\n",
        "            retrievers=[kiwi_bm25_retriever, faiss_retriever],\n",
        "            weights=[0.5, 0.5],\n",
        "            search_type=\"mmr\",\n",
        "        )\n",
        "\n",
        "        # 결과 저장\n",
        "        pdf_databases[pdf_title] = {\n",
        "                'db': db,\n",
        "                'retriever': retriever\n",
        "        }\n",
        "    return pdf_databases\n"
      ],
      "metadata": {
        "id": "hoU5L6p2Raot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if float(tab_ver[5:]) >= 2:\n",
        "  pkl_dir = os.path.join(temp_path,'tables')\n",
        "  tab_dict_trn = load_pkl(os.path.join(pkl_dir,'tab_trn.pkl'))\n",
        "  tab_dict_tst= load_pkl(os.path.join(pkl_dir,'tab_tst.pkl'))"
      ],
      "metadata": {
        "id": "0Ck5UcSlyGkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tab_word, tab_ver"
      ],
      "metadata": {
        "id": "mi6CMHVLIKqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbEdnYjmD47S"
      },
      "outputs": [],
      "source": [
        "if float(tab_ver[5:]) >= 2:\n",
        "  train_db, detect_rate = process_pdfs_from_df(train_df, temp_path, tab_dict_trn, tab_word, chunk_size=chunk_size, model_path=model_path)\n",
        "else : train_db = process_pdfs_from_dataframe(train_df, temp_path, chunk_size=chunk_size, model_path=model_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "free_cuda()"
      ],
      "metadata": {
        "id": "2Pma2NMoigpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if split_ver == 'split.2':\n",
        "  if float(tab_ver[5:]) >= 2:\n",
        "    valid_db, detect_rate = process_pdfs_from_df(valid_df, temp_path, tab_dict_trn, tab_word, chunk_size=chunk_size, model_path=model_path)\n",
        "  else : valid_db = process_pdfs_from_dataframe(valid_df, temp_path, chunk_size=chunk_size, model_path=model_path)\n",
        "else : valid_db = train_db"
      ],
      "metadata": {
        "id": "_FPllGNaTNcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aug_type"
      ],
      "metadata": {
        "id": "0lvJwUO_imWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if float(tab_ver[5:]) >= 2:\n",
        "  test_db, detect_rate = process_pdfs_from_df(test_df, temp_path, tab_dict_tst, tab_word[1:-1], chunk_size=chunk_size, model_path=model_path)\n",
        "else : test_db  = process_pdfs_from_dataframe(test_df, temp_path, chunk_size=chunk_size, model_path=model_path)"
      ],
      "metadata": {
        "id": "6QvE_rRVOk4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_dir"
      ],
      "metadata": {
        "id": "gg559fWhks8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db_name"
      ],
      "metadata": {
        "id": "e_27DwjfupE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#db_path = os.path.join('/content','pdf_db')\n",
        "db_path = os.path.join(file_dir,'pdf_db')\n",
        "#save_pkl(db_path, f'{db_name}_train.dat',train_db)\n",
        "#save_pkl(db_path, f'{db_name}_test.dat',test_db)"
      ],
      "metadata": {
        "id": "XVDqBYkHVl2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(db_path)"
      ],
      "metadata": {
        "id": "8q9gnAcStN9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_db_name = 'large-ensemble-tab_v1.7-256_train.dat'\n",
        "test_db_name = 'large-ensemble-tab_v1.7-256_test.dat'\n",
        "train_db_path = os.path.join(db_path,train_db_name)\n",
        "test_db_path = os.path.join(db_path,test_db_name)"
      ],
      "metadata": {
        "id": "-nvD0L2Wt6Ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = ' '.join([#train_db_path,\n",
        "            test_db_path\n",
        "                      ])\n",
        "temp_path = '/content/pdf_db'\n",
        "if not os.path.exists(temp_path) : os.makedirs(temp_path)"
      ],
      "metadata": {
        "id": "iepa-DnpuqcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rsync -vzh {file_path} {temp_path} --bwlimit 4096000000000000 --progress"
      ],
      "metadata": {
        "id": "S1XMFCgWt-Pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_db = load_pkl(os.path.join(temp_path,train_db_name))\n",
        "test_db = load_pkl(os.path.join(temp_path,test_db_name))"
      ],
      "metadata": {
        "id": "O6omkdKrtIPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4DLOeq7dBge"
      },
      "source": [
        "# Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dgTCr9ZhBaG"
      },
      "outputs": [],
      "source": [
        "def normalize_string(s):\n",
        "    \"\"\"유니코드 정규화\"\"\"\n",
        "    return unicodedata.normalize('NFC', s)\n",
        "\n",
        "def format_docs(docs):\n",
        "    \"\"\"검색된 문서들을 하나의 문자열로 포맷팅\"\"\"\n",
        "    context = \"\"\n",
        "    for doc in docs:\n",
        "        context += doc.page_content\n",
        "        context += '\\n'\n",
        "    return context\n",
        "\n",
        "def make_dataset(df, pdf_databases):\n",
        "    dataset = dict()\n",
        "    dataset['context'] = list()\n",
        "    dataset['question'] = list()\n",
        "    dataset['answer'] = list()\n",
        "    normalized_keys = {normalize_string(k): v for k, v in pdf_databases.items()}\n",
        "\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Making\"):\n",
        "        # 소스 문자열 정규화\n",
        "        source = normalize_string(row['Source'])+'.pdf'\n",
        "        question = row['Question']\n",
        "        dataset['question'].append(question)\n",
        "        if 'Answer' in df.columns:\n",
        "          dataset['answer'].append(row['Answer'])\n",
        "        else: dataset['answer'].append('')\n",
        "\n",
        "        # 정규화된 키로 데이터베이스 검색\n",
        "        retriever = normalized_keys[source]['retriever']\n",
        "        context = format_docs(retriever.invoke(question))\n",
        "        dataset['context'].append(context)\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "aKKFhbBIvXYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if aug_type != 'NoAug':\n",
        "  train_df = train_augged"
      ],
      "metadata": {
        "id": "IqshM3lVyOaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if (refine_option, filter_option) == (False, False) : prefix = ''\n",
        "elif (refine_option, filter_option) == (False, True) : prefix = 'filtered'\n",
        "elif (refine_option, filter_option) == (True, False) : prefix = 'refined0'\n",
        "elif (refine_option, filter_option) == (True, True) : prefix = 'refined'"
      ],
      "metadata": {
        "id": "I3zHLG1Aiae2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b20VnuxgiNd4"
      },
      "outputs": [],
      "source": [
        "dataset_name = \"kdt3/DACON-QA-{model}-ensemble-{tab_process}.{split_ver}-{prefix}{aug}-{chunck_size}\".format(prefix=prefix,**db_config)\n",
        "train_name = \"kdt3/DACON-QA-{model}-ensemble-{tab_process}.{split_ver}-{prefix}{aug}-{chunck_size}\".format(prefix=prefix,**db_config)\n",
        "#fname = \"gemma2_large_ensemble_markdown_256_5epoch_reprocessed_result.csv\"\n",
        "\n",
        "push_url = dataset_name\n",
        "push_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JB1KtlxYONn_"
      },
      "outputs": [],
      "source": [
        "## 만약 데이터셋을 분할해서 업로드해줘야할 경우 합치는 방법 참조 코드\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from datasets import Dataset\n",
        "\n",
        "train_dataset = load_dataset(dataset_name)['train']\n",
        "\n",
        "train_dataset = concatenate_datasets([train_dataset, Dataset.from_dict(make_dataset(train_df.iloc[296:], train_db))])\n",
        "train_dataset.push_to_hub(dataset_name, private=True, split='train')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_OUN6lHm_jO"
      },
      "source": [
        "## Train 데이터 생성 & 업로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wn-di5P-ZsXh"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "train_dataset = make_dataset(train_df, train_db)\n",
        "train_dataset = Dataset.from_dict(train_dataset)\n",
        "train_dataset.push_to_hub(push_url, private=True, split='train')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9G4ZhqpnEKd"
      },
      "source": [
        "## Valid 데이터 생성 & 업로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QG7qK-9vaupV"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "valid_dataset = make_dataset(valid_df, valid_db)\n",
        "valid_dataset = Dataset.from_dict(valid_dataset)\n",
        "valid_dataset.push_to_hub(push_url, private=True, split='valid')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kjq7wovnnGAX"
      },
      "source": [
        "## Test 데이터 생성 & 업로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Kn1WpOh-hHo"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "test_dataset = make_dataset(test_df, test_db)\n",
        "test_dataset = Dataset.from_dict(test_dataset)\n",
        "test_dataset.push_to_hub(push_url, private=True, split='test')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qzz0AVtDzMAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# debug\n",
        "-"
      ],
      "metadata": {
        "id": "MxQMzj63dApu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tables_from_pdf(full_path,tab_word='[[TABLE_{0}]]'):\n",
        "    pdf = pymupdf.open(full_path)\n",
        "    chunks, tables_dict, cnt= list(), defaultdict(list),0\n",
        "    err_dict = dict()\n",
        "    if pdf is None : return None, tables_dict\n",
        "    for pnum, page in enumerate(tqdm(pdf)):\n",
        "      if pnum < 7 : continue\n",
        "      return page\n",
        "      page_area = tuple(page.mediabox)\n",
        "      try:\n",
        "        tab_rslt = search_page(page,page_area,get_ths(page_area))\n",
        "        bboxes, errors = tab_rslt['tabs'],tab_rslt['errs']\n",
        "        tables,doc =list(), PyMuPDFPage(page)\n",
        "        formatter = define_formatter()\n",
        "        for box in bboxes:\n",
        "          table = make_table(box['bbox'],doc,page_area,formatter,get_ths(page_area))\n",
        "          if table is None :\n",
        "            print('error : ',box['bbox'])\n",
        "            continue\n",
        "          if type(table) is not dict : errors.append(table)\n",
        "          else :\n",
        "            table['depth'] = box['depth']\n",
        "            tables.append(table)\n",
        "\n",
        "      except:\n",
        "        save_pkl(ERRORDIR,'page_doc_error,pkl',[full_path,pnum,page_area])\n",
        "        raise Exception()\n",
        "      if len(errors)>0 :\n",
        "        err_dict[pnum] = errors\n",
        "        print('errs : ',len(errors))\n",
        "      if len(tables) == 0 : continue\n",
        "      print(f'detected in p.{pnum} :\\t',len(tables),' tables')\n",
        "      tables = sorted(tables,key=lambda x : (x['bbox'][0],x['bbox'][1]))\n",
        "      for idx,tab in enumerate(tables):\n",
        "        tab_mark = tab_word.format(cnt+idx)\n",
        "        table_md = clean_table(tab['content'].to_markdown(index=False))\n",
        "        tables_dict[pnum].append((tab_mark,table_md + f\"\\n{tab['caption']}\"))\n",
        "\n",
        "        try :\n",
        "          area = tab['bbox']\n",
        "          page.add_redact_annot(area)\n",
        "          page.apply_redactions()\n",
        "          page.draw_rect(area,color=(.0,0,0),fill=(.99,.99,.99))\n",
        "          rc = page.insert_htmlbox(area,tab_mark,scale_low=0)\n",
        "        except :\n",
        "          print(page.mediabox)\n",
        "          print(page.cropbox)\n",
        "          print(tab['bbox'])\n",
        "          display(tab['content'])\n",
        "          print(table_md)\n",
        "\n",
        "      cnt+=len(tables)\n",
        "\n",
        "    print(cnt, len(tables_dict), sum([len(tables) for tables in tables_dict.values()]))\n",
        "    return pdf, tables_dict,err_dict\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HecotK5SS-TF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = '''\n",
        "index,,\n",
        "0,\"02 재정지출\",\"104\"\n",
        "1,\"1. 국가별\",\"104\"\n",
        "2,\"2. 기능별 재정지출\",\"106\"\n",
        "3,\"3. 공공 사회 복지 지출\",\"107\"\n",
        "4,\"03 재정건전성\",\"108\"\n",
        "5,\"1. 재정수지\",\"108\"\n",
        "6,\"(1) 재정수지\",\"108\"\n",
        "7,\"(2) 기초 재정수지\",\"109\"\n",
        "8,\"2. 부채\",\"110\"\n",
        "9,\"(1) 부채\",\"110\"\n",
        "10,\"(2) 1인당 부채\",\"111\"\n",
        "11,\"주요재정통계\",\"113\"\n",
        "12,\"01 총수입･국세수입\",\"116\"\n",
        "13,\"02 조세부담률 및 국민부담률\",\"118\"\n",
        "14,\"03 총지출\",\"120\"\n",
        "15,\"04 분야별 재정지출\",\"122\"\n",
        "16,\"05 의무지출･재량지출 통합재정수지･국가채무\",\"124\"\n",
        "17,\"06\",\"126\"\n",
        "18,\"07 지방재정조정\",\"126\"\n",
        "'''\n",
        "import sys\n",
        "if sys.version_info[0] < 3:\n",
        "    from StringIO import StringIO\n",
        "else:\n",
        "    from io import StringIO\n",
        "\n",
        "content = StringIO(s)\n",
        "df = pd.read_table(content, sep=\",\",index_col=0)\n",
        "display(df.info())\n",
        "check_table_df_soundness(df)"
      ],
      "metadata": {
        "id": "kCuaGUfJMqxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df,base_path,save_dir = train_df, temp_path,PROCESSEDDIR\n",
        "path = './train_source/1-1 2024 주요 재정통계 1권.pdf'\n",
        "tab_rslt,err_rslt=extract_table_and_pdf(path,base_path,save_dir)"
      ],
      "metadata": {
        "id": "JxhOCpGYofNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = temp_path\n",
        "file_path = './train_source/1-1 2024 주요 재정통계 1권.pdf'\n",
        "full_path = process_path(base_dir,file_path)\n",
        "full_path = processed_path_matcher(base_dir,full_path)\n",
        "\n",
        "page = get_tables_from_pdf(full_path)"
      ],
      "metadata": {
        "id": "G4A8PeEtTlLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_page(page,area,ths=0,depth=0):\n",
        "  if not larger_v_ths(area,ths) : return defaultdict(list)\n",
        "  if depth >=10 : raise Exception(depth)\n",
        "#  print('depth : ',depth,'area : ',area)\n",
        "  page.set_cropbox(area) #area : page에서 절대적 위치. cropbox를 하게 되면 상대적 위치로 바뀜\n",
        "  rslt,searched = defaultdict(list),list()\n",
        "#  set_trace()\n",
        "  detected = find_tables(page,get_page_size(area),ths)\n",
        "#  return detected\n",
        "  if len(detected)==0 : return rslt\n",
        "  for target in detected:\n",
        "    if target is None : continue\n",
        "    if type(target) is not dict : rslt['errs'].append(target)\n",
        "    rect = infer_bbox_pos(area,target['bbox'])\n",
        "    bbox = {'bbox':rect,'depth':depth}\n",
        "    rslt['tabs'].append(bbox)\n",
        "    print('detected:',rect,'\\t at',area,f' in depth {depth}')\n",
        "\n",
        "    left_area = get_area(rect,area,True)\n",
        "    right_area = get_area(rect,area,False)\n",
        "    if depth > 5 : print(f'left {left_area}\\tright{right_area}')\n",
        "    left = search_page(page,left_area,ths,depth+1)\n",
        "    right = search_page(page,right_area,ths,depth+1)\n",
        "    searched.append((area[0],rect[1],area[2],rect[3]))\n",
        "    if depth > 5 : print(searched)\n",
        "    extend_list_dict(rslt,left)\n",
        "    extend_list_dict(rslt,right)\n",
        "\n",
        "  if len(rslt)==0 : return rslt\n",
        "  searched = organize_box(searched,area,ths)\n",
        "  blanks = get_blank(searched,area,ths)\n",
        "  if depth >5 : print(f'in {area}\\n\\t',blanks)\n",
        "  for row in blanks:\n",
        "    detected = search_page(page,row,ths,depth+1)\n",
        "    extend_list_dict(rslt,detected)\n",
        "\n",
        "#  rslt= organize_box(rslt,area,ths) : can't apply directly like this\n",
        "#  print('in search page, err : ',len(rslt['errs']))\n",
        "  page.set_cropbox(area)\n",
        "  return rslt"
      ],
      "metadata": {
        "id": "McvfpuVjV85p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_page(page,page.mediabox)"
      ],
      "metadata": {
        "id": "1mUkEcL5UWue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check_table_df_soundness(detected[0])\n",
        "df_temp = detected[0].replace(to_replace=[None], value=0).astype(str)\n",
        "df_temp = df_temp.fillna('').astype(str)\n",
        "val = '0'\n",
        "cols =range(len(df_temp.columns))\n",
        "df_temp.columns =cols\n",
        "cond = df_temp == val\n",
        "for col in cols:\n",
        "  display(np.sum(cond[col]))\n",
        "#target = list(filter(lambda col : np.sum(cond[col]) != len(cond),cols))\n",
        "#erase_constant_rowcol(df_temp,'0')"
      ],
      "metadata": {
        "id": "fAbw8YEEWGMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls {PROCESSEDDIR}/train_source"
      ],
      "metadata": {
        "id": "Sf1pLurE1Mr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "errs = load_pkl(os.path.join(PROCESSEDDIR,'train_source','err_1-1 2024 주요 재정통계 1권.pkl'))\n",
        "len(errs)"
      ],
      "metadata": {
        "id": "6OLCHTE4oB8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "errs[6]"
      ],
      "metadata": {
        "id": "vwSmMade9gD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_table_df_soundness(df,ths=0.5):\n",
        "  if len(df) < 1 : return False\n",
        "  df_temp = df.replace(to_replace=None, value=0).astype(str)\n",
        "  if maybe_numeric_table(df_temp) : return True\n",
        "  df_temp = df.replace(to_replace=None, value='PD_NONE')\n",
        "  cols = list(df.columns.astype(np.string_))\n",
        "  cols = list(map(lambda x : '' if x is None else str(x),cols))\n",
        "  cols = list(map(lambda x : re.sub(r'[\\s]*','',x),cols))\n",
        "  null_col = list(filter(lambda x: len(x)<1,cols))\n",
        "  if len(null_col) > len(cols) * ths : return False\n",
        "  if np.sum(df_temp=='PD_NONE') > len(df)*len(cols)*ths : return False\n",
        "  return True\n",
        "\n",
        "\n",
        "def maybe_numeric_table(df,ths=0.35):\n",
        "  df_temp = df.fillna('').astype(str)\n",
        "  df_temp = df.replace(r'(?:(\\d+?)),(\\d+?)',r'\\1\\2',regex=True)\n",
        "  df_temp = df_temp[list(df_temp.columns)].apply(pd.to_numeric,errors='coerce')\n",
        "  rslt = ~df_temp.isna()\n",
        "  rowwise = rslt.apply(sum,axis=1)\n",
        "  colwise = rslt.apply(sum,axis=0)\n",
        "  if np.sum(rowwise == len(rslt)) > 0 : return True\n",
        "  if np.sum(colwise== len(rslt.columns)) > 0 : return True\n",
        "  if np.sum(rslt.values) > len(rslt)*len(rslt.columns)*ths : return True\n",
        "  return False"
      ],
      "metadata": {
        "id": "4Kck0Kc-wmRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bad = [{\"index\":0,\"\":\"2018년부터 발간한 ｢주요 재정통계｣는 디지털예산회계시스템의 재정정보를\"},{\"index\":1,\"\":\"계에 따른 시계열 통계로 구성･제공하고 있습니다. 아울러 중앙-지방정부 등 여러\"},{\"index\":2,\"\":\"산재되어 있는 재정통계를 수록하여 재정분석의 기초자료로 활용될 수 있도록\"},{\"index\":3,\"\":\"다. 또한 단순 정보전달에 그치지 않고, 일반 국민도 쉽게 이해할 수 있도록 재정통계의 가독성을 높이고자 노력하였습니다.\"},{\"index\":4,\"\":\"특히, 올해부터는 한권으로 제공되던 ｢주요 재정통계｣를 Ⅰ, Ⅱ 권으로 분권하여,\"},{\"index\":5,\"\":\"편의성을 제고하고자 하였습니다. <제Ⅰ권>에서 우리나라의 재정체계, 주요 재정지표\"},{\"index\":6,\"\":\"재정 전반에 대한 이해를 돕기 위해 주요 재정 제도의 설명과 함께 관련 통계를 수록하였고,\"},{\"index\":7,\"\":\"OECD, IMF 회원국 간 재정 동향을 비교･분석할 수 있는 통계로 구성하였습니다.\"},{\"index\":8,\"\":\"<제Ⅱ권>에서는 예산체계에 따른 16대 분야별 재정 구조와 추이, 사업유형별 주요사업 정보를 담아 각 분야별 재정지출에 대한 이해도를 높일 수 있도록 구성하였습니다. 이와 함께, 부록에서는 국내 핵심 재정 통계를 선정하여 10년 이상의 장기 시계열\"},{\"index\":9,\"\":\"표를 추가 제공하였습니다.\"},{\"index\":10,\"\":\"한국재정정보원은 ｢2024 주요 재정통계｣가 국민들의 재정에 대한 이해도를\"},{\"index\":11,\"\":\"재정당국에게는 재정정책 수립의 기초자료로 활용되길 바랍니다. 앞으로 시의적절함과\"},{\"index\":12,\"\":\"동시에 정합성을 담보한 다양한 재정자료를 지속적으로 제공할 수 있도록 최선을 습니다.\"},{\"index\":13,\"\":\"2024년\"}]\n",
        "df = pd.DataFrame(bad)\n",
        "df_temp = df.replace(to_replace=[None], value=0).astype(str)\n",
        "df_temp = df_temp.fillna('').astype(str)\n",
        "df_temp = df.replace(r'(?:(\\d+?)),(\\d+?)',r'\\1\\2',regex=True)\n",
        "df_temp = df_temp[list(df_temp.columns)].apply(pd.to_numeric,errors='coerce')\n",
        "rslt = ~df_temp.isna()\n",
        "rowwise = rslt.apply(sum,axis=1)\n",
        "colwise = rslt.apply(sum,axis=0)"
      ],
      "metadata": {
        "id": "PUN8yEY2v0vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(rowwise)"
      ],
      "metadata": {
        "id": "_aujjLVKxxt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols"
      ],
      "metadata": {
        "id": "ZUMbbHjVx5Av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rslt"
      ],
      "metadata": {
        "id": "PmAf6YKvx27J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(rslt)"
      ],
      "metadata": {
        "id": "p_-vU2n-xve9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_temp = df.replace(to_replace=[None], value=0).astype(str)\n",
        "if maybe_numeric_table(df_temp) : print(True)"
      ],
      "metadata": {
        "id": "bLcKc3CtwP6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path"
      ],
      "metadata": {
        "id": "09U2zn92p-n4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "LHsNBioph_AG",
        "AFuh6k7hD47P",
        "h9G4ZhqpnEKd"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}