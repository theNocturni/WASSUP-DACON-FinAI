[DACON 재정정보 AI 검색 알고리즘 경진대회] (https://dacon.io/competitions/official/236295/overview/description) 참여 코드

구성원: 박광준, 신수웅, 오도은

## 1. 개요

faiss 기반 retriver와 multilingual-E5기반 embedding 모델로 vector DB를 구축하고, gemma2 기반 LLM 모델 학습

### 배경

- 정부에선 매해 사업기획서, 재정통계, 재정보고서 등 다양한 재정 관련 정보를 축적
- 이런 방대한 양의 재정 데이터에 대해 쉽게 접근 가능하도록 하여 재정 데이터의 제공 편의성과 활용도를 개선하기 위한 프로젝트

## 2. 주제

- 열린재정의 중앙정부 재정 정보 검색 및 제공 편의성/활용도를 높이는 질의 응답 AI 알고리즘 개발
- 제공된 재정 관련 텍스트 데이터를 활용해 주어진 질문에 대해 정확도가 높은 응답을 제시하는 자연어처리 알고리즘 개발

## 3. 데이터셋

### 1) 문서 데이터셋

- Train set: 16개의 정부 사업기획서 및 한국재정정보원에서 발간하는 다양한 분석보고서, 통계자료집 등의 발간물
- Test set: 9개의 정부 사업기획서 및 한국재정정보원에서 발간하는 다양한 분석보고서, 통계자료집 등의 발간물

### 2) 질의 응답 데이터셋

- Train set: 496개의 질의응답 데이터가 포함된 csv 파일
    -각 행은 질문에 대한 연관문서, 질문, 그에 대한 답변으로 이루어져 있음
- Test set: 98개의 관련 문서와 질문이 포함된 csv 파일
    -각 행은 답변 없이 질문에 대한 연관문서 와 질문으로 이루어져 있음

## 4. 전략

크게 RAG, 추가학습, 프롬프팅의 3가지 측면으로 접근

### RAG

### 추가학습

- 한국어에 대한 사전학습이 진행된 "rtzr/ko-gemma-2-9b-it" 모델을 기반으로 작업
- Instruction Finetuning: 사전학습이 된 모델의 경우 이미 언어에 대한 기본적인 이해도를 지니고 있기에, 수행하고자 하는 작업에 따라 그 작업에 대한 지시사항을 학습데이터에 포함하여 목적에 대해 더 효과적인 학습을 진행할 수 있음
- LoRA(Low-Rank Adaptation): LLM 전체에 대해 추가학습을 진행하려 할 경우 개인 사용자 입장에서는 너무 많은 컴퓨팅 자원을 소모하게 됨. 이때 모델의 가중치는 동결시키되 원 모델 레이어에 LoRA 어댑터를 적용한 뒤 LoRA 레이어만 학습시켜 전체 모델의 추가학습과 유사한 효과를 얻으면서도 자원 소모를 획기적으로 줄일 수 있음

### 프롬프팅

## 5. 최종 코드 구조

4비트 양자화한 "rtzr/ko-gemma-2-9b-it" 모델에 대하여 LoRA 추가학습 기법을 사용해 3에포크 추가학습 진행


    
## 대회 제출 결과
Public 리더보드 0.66626
Private 리더보드 0.67343

최종순위 38/359 (상위 10.58%)
